"""
ì˜¤í”ˆì†ŒìŠ¤ LLM ìµœì í™” ê²°ê³¼ ì‹œê°í™” ë° ë¶„ì„ ì‹œìŠ¤í…œ
"""
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

class ResultVisualizer:
    """ìµœì í™” ê²°ê³¼ ì‹œê°í™” í´ë˜ìŠ¤"""

    def __init__(self, results_dir: str = "optimization_results"):
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)

        # ì‹œê°í™” ì €ì¥ ë””ë ‰í† ë¦¬
        self.viz_dir = Path("visualizations")
        self.viz_dir.mkdir(exist_ok=True)

    def load_optimization_results(self, pattern: str = "opt_*.json") -> List[Dict[str, Any]]:
        """ìµœì í™” ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë“œ"""
        results = []

        for filepath in self.results_dir.glob(pattern):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                    results.append(result)
            except Exception as e:
                print(f"íŒŒì¼ {filepath} ë¡œë“œ ì‹¤íŒ¨: {e}")

        return results

    def load_benchmark_results(self, pattern: str = "bench_*.json") -> List[Dict[str, Any]]:
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë“œ"""
        results = []

        for filepath in self.results_dir.glob(pattern):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                    results.append(result)
            except Exception as e:
                print(f"íŒŒì¼ {filepath} ë¡œë“œ ì‹¤íŒ¨: {e}")

        return results

    def load_all_results(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
        return self.load_optimization_results("*.json")

    def create_optimization_analysis(self, results: List[Dict[str, Any]],
                                   save_path: str = None) -> go.Figure:
        """ìµœì í™” ê²°ê³¼ ë¶„ì„ ì°¨íŠ¸ ìƒì„±"""
        if not results:
            return None

        # ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                'ëª¨ë¸ë³„ ìµœì í™” ì„±ëŠ¥',
                'íŒŒë¼ë¯¸í„° ìµœì í™” íŠ¸ë Œë“œ',
                'í•˜ë“œì›¨ì–´ ì‚¬ìš©ëŸ‰ ë¶„ì„',
                'íŒŒë¼ë¯¸í„° ìƒê´€ê´€ê³„'
            ),
            specs=[[{"type": "bar"}, {"type": "scatter"}],
                   [{"type": "heatmap"}, {"type": "scatter"}]]
        )

        # 1. ëª¨ë¸ë³„ ì„±ëŠ¥ ë§‰ëŒ€ ì°¨íŠ¸
        model_scores = {}
        for result in results:
            model = result.get('model_name', 'Unknown')
            score = result.get('best_score', 0)
            if model not in model_scores or score > model_scores[model]:
                model_scores[model] = score

        fig.add_trace(
            go.Bar(
                x=list(model_scores.keys()),
                y=list(model_scores.values()),
                name='ìµœê³  ì ìˆ˜',
                marker_color='lightblue',
                showlegend=False
            ),
            row=1, col=1
        )

        # 2. ìµœì í™” íŠ¸ë Œë“œ (ì‹œê°„ë³„)
        if len(results) > 1:
            timestamps = []
            scores = []
            models = []

            for result in sorted(results, key=lambda x: x.get('timestamp', '')):
                timestamps.append(result.get('timestamp', ''))
                scores.append(result.get('best_score', 0))
                models.append(result.get('model_name', 'Unknown'))

            fig.add_trace(
                go.Scatter(
                    x=list(range(len(timestamps))),
                    y=scores,
                    mode='lines+markers',
                    name='ì„±ëŠ¥ íŠ¸ë Œë“œ',
                    text=models,
                    hovertemplate='<b>%{text}</b><br>ì ìˆ˜: %{y:.3f}<extra></extra>',
                    showlegend=False
                ),
                row=1, col=2
            )

        # 3. í•˜ë“œì›¨ì–´ ì‚¬ìš©ëŸ‰ íˆíŠ¸ë§µ
        hw_data = []
        hw_models = []
        hw_metrics = ['cpu_avg', 'memory_peak', 'gpu_memory_peak']

        for result in results:
            if 'hardware_usage' in result:
                hw = result['hardware_usage']
                model = result.get('model_name', 'Unknown')
                hw_models.append(model)
                hw_data.append([
                    hw.get('cpu_avg', 0),
                    hw.get('memory_peak', 0),
                    hw.get('gpu_memory_peak', 0)
                ])

        if hw_data:
            hw_array = np.array(hw_data).T
            fig.add_trace(
                go.Heatmap(
                    z=hw_array,
                    x=hw_models,
                    y=['CPU (%)', 'ë©”ëª¨ë¦¬ (%)', 'GPU ë©”ëª¨ë¦¬ (GB)'],
                    colorscale='Viridis',
                    showscale=False
                ),
                row=2, col=1
            )

        # 4. íŒŒë¼ë¯¸í„° ìƒê´€ê´€ê³„ (Temperature vs Top-p)
        temperatures = []
        top_ps = []
        scores = []
        model_names = []

        for result in results:
            best_params = result.get('best_params', {})
            temperatures.append(best_params.get('temperature', 0))
            top_ps.append(best_params.get('top_p', 0))
            scores.append(result.get('best_score', 0))
            model_names.append(result.get('model_name', 'Unknown'))

        fig.add_trace(
            go.Scatter(
                x=temperatures,
                y=top_ps,
                mode='markers',
                marker=dict(
                    size=10,
                    color=scores,
                    colorscale='Viridis',
                    showscale=True,
                    colorbar=dict(title="ì ìˆ˜")
                ),
                text=model_names,
                hovertemplate='<b>%{text}</b><br>Temperature: %{x}<br>Top-p: %{y}<br>ì ìˆ˜: %{marker.color:.3f}<extra></extra>',
                showlegend=False
            ),
            row=2, col=2
        )

        # ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸
        fig.update_layout(
            title='LLM ìµœì í™” ê²°ê³¼ ì¢…í•© ë¶„ì„',
            height=800,
            showlegend=False
        )

        # ì¶• ë ˆì´ë¸” ì„¤ì •
        fig.update_xaxes(title_text="ëª¨ë¸", row=1, col=1)
        fig.update_yaxes(title_text="ì ìˆ˜", row=1, col=1)

        fig.update_xaxes(title_text="ìµœì í™” ìˆœì„œ", row=1, col=2)
        fig.update_yaxes(title_text="ì ìˆ˜", row=1, col=2)

        fig.update_xaxes(title_text="Temperature", row=2, col=2)
        fig.update_yaxes(title_text="Top-p", row=2, col=2)

        if save_path:
            fig.write_html(self.viz_dir / f"{save_path}.html")
            try:
                fig.write_image(self.viz_dir / f"{save_path}.png")
            except:
                pass  # kaleidoê°€ ì—†ì–´ë„ HTMLì€ ì €ì¥ë¨

        return fig

    def create_benchmark_analysis(self, results: List[Dict[str, Any]],
                                save_path: str = None) -> go.Figure:
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„ ì°¨íŠ¸ ìƒì„±"""
        if not results:
            return None

        # ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                'ëª¨ë¸ë³„ ì²˜ë¦¬ ì†ë„',
                'ì§€ì—°ì‹œê°„ ë¶„í¬',
                'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ vs ì„±ëŠ¥',
                'ë¹„ìš© íš¨ìœ¨ì„±'
            ),
            specs=[[{"type": "bar"}, {"type": "box"}],
                   [{"type": "scatter"}, {"type": "bar"}]]
        )

        # ë°ì´í„° ì¤€ë¹„
        models = []
        tokens_per_sec = []
        latencies = []
        memory_usage = []
        costs = []

        for result in results:
            model = result.get('model_name', 'Unknown')
            perf = result.get('performance_metrics', {})
            cost = result.get('cost_analysis', {})

            models.append(model)
            tokens_per_sec.append(perf.get('tokens_per_second', 0))
            latencies.append([
                perf.get('latency_p50', 0),
                perf.get('latency_p95', 0),
                perf.get('latency_p99', 0)
            ])
            memory_usage.append(perf.get('memory_usage_mb', 0))
            costs.append(cost.get('cost_per_1k_tokens_usd', 0))

        # 1. ì²˜ë¦¬ ì†ë„ ë§‰ëŒ€ ì°¨íŠ¸
        fig.add_trace(
            go.Bar(
                x=models,
                y=tokens_per_sec,
                name='í† í°/ì´ˆ',
                marker_color='lightgreen',
                showlegend=False
            ),
            row=1, col=1
        )

        # 2. ì§€ì—°ì‹œê°„ ë°•ìŠ¤ í”Œë¡¯
        for i, model in enumerate(models):
            fig.add_trace(
                go.Box(
                    y=latencies[i],
                    name=model,
                    showlegend=False
                ),
                row=1, col=2
            )

        # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ vs ì„±ëŠ¥ ì‚°ì ë„
        fig.add_trace(
            go.Scatter(
                x=memory_usage,
                y=tokens_per_sec,
                mode='markers+text',
                text=models,
                textposition='top center',
                marker=dict(size=12, color='orange'),
                name='ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±',
                showlegend=False
            ),
            row=2, col=1
        )

        # 4. ë¹„ìš© íš¨ìœ¨ì„±
        if any(costs):
            fig.add_trace(
                go.Bar(
                    x=models,
                    y=costs,
                    name='1Kí† í°ë‹¹ ë¹„ìš©($)',
                    marker_color='red',
                    showlegend=False
                ),
                row=2, col=2
            )

        # ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸
        fig.update_layout(
            title='LLM ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„',
            height=800
        )

        # ì¶• ë ˆì´ë¸” ì„¤ì •
        fig.update_xaxes(title_text="ëª¨ë¸", row=1, col=1)
        fig.update_yaxes(title_text="í† í°/ì´ˆ", row=1, col=1)

        fig.update_yaxes(title_text="ì§€ì—°ì‹œê°„ (ì´ˆ)", row=1, col=2)

        fig.update_xaxes(title_text="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)", row=2, col=1)
        fig.update_yaxes(title_text="í† í°/ì´ˆ", row=2, col=1)

        fig.update_xaxes(title_text="ëª¨ë¸", row=2, col=2)
        fig.update_yaxes(title_text="ë¹„ìš© ($)", row=2, col=2)

        if save_path:
            fig.write_html(self.viz_dir / f"{save_path}.html")
            try:
                fig.write_image(self.viz_dir / f"{save_path}.png")
            except:
                pass

        return fig

    def create_model_comparison_chart(self, results: List[Dict[str, Any]],
                                    save_path: str = None) -> go.Figure:
        """ëª¨ë¸ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
        if not results:
            return None

        # ëª¨ë¸ë³„ ìµœê³  ì„±ëŠ¥ ì§‘ê³„
        model_data = {}

        for result in results:
            model = result.get('model_name', 'Unknown')

            if model not in model_data:
                model_data[model] = {
                    'best_score': 0,
                    'best_speed': 0,
                    'min_memory': float('inf'),
                    'min_cost': float('inf'),
                    'datasets': set()
                }

            # ìµœì í™” ê²°ê³¼
            if 'best_score' in result:
                score = result['best_score']
                if score > model_data[model]['best_score']:
                    model_data[model]['best_score'] = score

            # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
            if 'performance_metrics' in result:
                perf = result['performance_metrics']
                speed = perf.get('tokens_per_second', 0)
                memory = perf.get('memory_usage_mb', float('inf'))

                if speed > model_data[model]['best_speed']:
                    model_data[model]['best_speed'] = speed

                if memory < model_data[model]['min_memory']:
                    model_data[model]['min_memory'] = memory

            # ë¹„ìš© ì •ë³´
            if 'cost_analysis' in result:
                cost = result['cost_analysis'].get('cost_per_1k_tokens_usd', float('inf'))
                if cost < model_data[model]['min_cost']:
                    model_data[model]['min_cost'] = cost

            # ë°ì´í„°ì…‹ ì •ë³´
            dataset = result.get('dataset_name', '')
            if dataset:
                model_data[model]['datasets'].add(dataset)

        # ë ˆì´ë” ì°¨íŠ¸ ìƒì„±
        fig = go.Figure()

        categories = ['ì •í™•ë„', 'ì†ë„', 'ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±', 'ë¹„ìš© íš¨ìœ¨ì„±', 'ë²”ìš©ì„±']

        for model, data in model_data.items():
            # ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚°
            accuracy_score = data['best_score']
            speed_score = min(data['best_speed'] / 100, 1.0)  # 100 tokens/sec = 1.0
            memory_score = max(0, 1.0 - (data['min_memory'] / 10000))  # 10GB = 0ì 
            cost_score = max(0, 1.0 - (data['min_cost'] * 10000))  # ë‚®ì€ ë¹„ìš©ì´ ë†’ì€ ì ìˆ˜
            versatility_score = min(len(data['datasets']) / 5, 1.0)  # 5ê°œ ë°ì´í„°ì…‹ = 1.0

            values = [accuracy_score, speed_score, memory_score, cost_score, versatility_score]

            fig.add_trace(go.Scatterpolar(
                r=values,
                theta=categories,
                fill='toself',
                name=model,
                line=dict(width=2)
            ))

        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )
            ),
            title='ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥ ë¹„êµ',
            showlegend=True,
            height=600
        )

        if save_path:
            fig.write_html(self.viz_dir / f"{save_path}.html")
            try:
                fig.write_image(self.viz_dir / f"{save_path}.png")
            except:
                pass

        return fig

    def create_hardware_analysis(self, hardware_info: Dict[str, Any],
                               save_path: str = None) -> go.Figure:
        """í•˜ë“œì›¨ì–´ ë¶„ì„ ì°¨íŠ¸ ìƒì„±"""

        # ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                'GPU ë©”ëª¨ë¦¬ í˜„í™©',
                'ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ',
                'ì¶”ì²œ ëª¨ë¸ í¬ê¸°',
                'ì„±ëŠ¥ ì˜ˆì¸¡'
            ),
            specs=[[{"type": "bar"}, {"type": "pie"}],
                   [{"type": "bar"}, {"type": "scatter"}]]
        )

        # 1. GPU ë©”ëª¨ë¦¬ í˜„í™©
        if hardware_info.get('cuda_available', False):
            gpu_names = []
            gpu_memories = []

            for i in range(hardware_info['cuda_device_count']):
                gpu_name = hardware_info.get(f'gpu_{i}_name', f'GPU {i}')
                gpu_memory = hardware_info.get(f'gpu_{i}_memory', 0)
                gpu_names.append(gpu_name)
                gpu_memories.append(gpu_memory)

            fig.add_trace(
                go.Bar(
                    x=gpu_names,
                    y=gpu_memories,
                    name='GPU ë©”ëª¨ë¦¬ (GB)',
                    marker_color='lightblue',
                    showlegend=False
                ),
                row=1, col=1
            )

        # 2. ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        total_memory = hardware_info.get('total_memory', 0)
        available_memory = hardware_info.get('available_memory', 0)
        used_memory = total_memory - available_memory

        fig.add_trace(
            go.Pie(
                labels=['ì‚¬ìš© ì¤‘', 'ì‚¬ìš© ê°€ëŠ¥'],
                values=[used_memory, available_memory],
                showlegend=False
            ),
            row=1, col=2
        )

        # 3. ì¶”ì²œ ëª¨ë¸ í¬ê¸°
        total_gpu_memory = sum(
            hardware_info.get(f'gpu_{i}_memory', 0)
            for i in range(hardware_info.get('cuda_device_count', 0))
        )

        model_recommendations = self._get_model_recommendations(total_gpu_memory)

        fig.add_trace(
            go.Bar(
                x=list(model_recommendations.keys()),
                y=list(model_recommendations.values()),
                name='ì¶”ì²œ ì ìˆ˜',
                marker_color='lightgreen',
                showlegend=False
            ),
            row=2, col=1
        )

        # 4. ì„±ëŠ¥ ì˜ˆì¸¡
        model_sizes = ['7B', '13B', '30B', '70B']
        predicted_speeds = self._predict_performance(hardware_info, model_sizes)

        fig.add_trace(
            go.Scatter(
                x=model_sizes,
                y=predicted_speeds,
                mode='lines+markers',
                name='ì˜ˆìƒ ì†ë„',
                line=dict(color='orange', width=3),
                marker=dict(size=8),
                showlegend=False
            ),
            row=2, col=2
        )

        # ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸
        fig.update_layout(
            title='í•˜ë“œì›¨ì–´ ë¶„ì„ ë° ì„±ëŠ¥ ì˜ˆì¸¡',
            height=800
        )

        # ì¶• ë ˆì´ë¸” ì„¤ì •
        fig.update_xaxes(title_text="GPU", row=1, col=1)
        fig.update_yaxes(title_text="ë©”ëª¨ë¦¬ (GB)", row=1, col=1)

        fig.update_xaxes(title_text="ëª¨ë¸ í¬ê¸°", row=2, col=1)
        fig.update_yaxes(title_text="ì¶”ì²œ ì ìˆ˜", row=2, col=1)

        fig.update_xaxes(title_text="ëª¨ë¸ í¬ê¸°", row=2, col=2)
        fig.update_yaxes(title_text="ì˜ˆìƒ ì†ë„ (tokens/sec)", row=2, col=2)

        if save_path:
            fig.write_html(self.viz_dir / f"{save_path}.html")
            try:
                fig.write_image(self.viz_dir / f"{save_path}.png")
            except:
                pass

        return fig

    def _get_model_recommendations(self, total_gpu_memory: float) -> Dict[str, float]:
        """GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ëª¨ë¸ ì¶”ì²œ ì ìˆ˜"""
        recommendations = {}

        if total_gpu_memory >= 160:
            recommendations.update({'70B': 1.0, '30B': 1.0, '13B': 1.0, '7B': 1.0})
        elif total_gpu_memory >= 80:
            recommendations.update({'70B': 0.7, '30B': 1.0, '13B': 1.0, '7B': 1.0})
        elif total_gpu_memory >= 32:
            recommendations.update({'70B': 0.3, '30B': 0.8, '13B': 1.0, '7B': 1.0})
        elif total_gpu_memory >= 16:
            recommendations.update({'70B': 0.0, '30B': 0.2, '13B': 0.7, '7B': 1.0})
        elif total_gpu_memory >= 8:
            recommendations.update({'70B': 0.0, '30B': 0.0, '13B': 0.3, '7B': 0.8})
        else:
            recommendations.update({'70B': 0.0, '30B': 0.0, '13B': 0.0, '7B': 0.5})

        return recommendations

    def _predict_performance(self, hardware_info: Dict[str, Any], model_sizes: List[str]) -> List[float]:
        """ëª¨ë¸ í¬ê¸°ë³„ ì„±ëŠ¥ ì˜ˆì¸¡"""
        total_gpu_memory = sum(
            hardware_info.get(f'gpu_{i}_memory', 0)
            for i in range(hardware_info.get('cuda_device_count', 0))
        )

        # ê°„ë‹¨í•œ ì„±ëŠ¥ ì˜ˆì¸¡ ëª¨ë¸ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ëª¨ë¸ ì‚¬ìš©)
        base_performance = min(total_gpu_memory * 5, 100)  # GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ê¸°ë³¸ ì„±ëŠ¥

        predictions = []
        for size in model_sizes:
            if size == '7B':
                pred = base_performance * 0.9
            elif size == '13B':
                pred = base_performance * 0.7
            elif size == '30B':
                pred = base_performance * 0.4
            elif size == '70B':
                pred = base_performance * 0.2
            else:
                pred = base_performance * 0.5

            predictions.append(max(pred, 0))

        return predictions

    def create_comprehensive_dashboard(self, save_path: str = "dashboard") -> go.Figure:
        """ì¢…í•© ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        # ëª¨ë“  ê²°ê³¼ ë¡œë“œ
        optimization_results = self.load_optimization_results()
        benchmark_results = self.load_benchmark_results()

        # í° ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig = make_subplots(
            rows=3, cols=3,
            subplot_titles=(
                'ìµœì í™” ì„±ëŠ¥ íŠ¸ë Œë“œ', 'ëª¨ë¸ë³„ ì²˜ë¦¬ ì†ë„', 'íŒŒë¼ë¯¸í„° ë¶„í¬',
                'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰', 'ë¹„ìš© ë¶„ì„', 'ì •í™•ë„ vs ì†ë„',
                'í•˜ë“œì›¨ì–´ íš¨ìœ¨ì„±', 'ë°ì´í„°ì…‹ë³„ ì„±ëŠ¥', 'ì¢…í•© ì ìˆ˜'
            ),
            specs=[[{"type": "scatter"}, {"type": "bar"}, {"type": "histogram"}],
                   [{"type": "box"}, {"type": "bar"}, {"type": "scatter"}],
                   [{"type": "heatmap"}, {"type": "bar"}, {"type": "polar"}]]
        )

        # ê° ì°¨íŠ¸ì— ë°ì´í„° ì¶”ê°€ (ê°„ì†Œí™”)
        if optimization_results:
            # 1. ìµœì í™” íŠ¸ë Œë“œ
            scores = [r.get('best_score', 0) for r in optimization_results]
            fig.add_trace(
                go.Scatter(y=scores, mode='lines+markers', name='ìµœì í™” ì ìˆ˜', showlegend=False),
                row=1, col=1
            )

        if benchmark_results:
            # 2. ëª¨ë¸ë³„ ì†ë„
            models = [r.get('model_name', 'Unknown') for r in benchmark_results]
            speeds = [r.get('performance_metrics', {}).get('tokens_per_second', 0) for r in benchmark_results]
            fig.add_trace(
                go.Bar(x=models, y=speeds, name='ì²˜ë¦¬ ì†ë„', showlegend=False),
                row=1, col=2
            )

        # ì¶”ê°€ ì°¨íŠ¸ë“¤ë„ ìœ ì‚¬í•˜ê²Œ êµ¬í˜„...

        fig.update_layout(
            title='LLM ìµœì í™” ì¢…í•© ëŒ€ì‹œë³´ë“œ',
            height=1200,
            showlegend=False
        )

        if save_path:
            fig.write_html(self.viz_dir / f"{save_path}.html")
            try:
                fig.write_image(self.viz_dir / f"{save_path}.png")
            except:
                pass

        return fig

    def generate_performance_report(self, save_path: str = "performance_report.html"):
        """ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        optimization_results = self.load_optimization_results()
        benchmark_results = self.load_benchmark_results()

        # HTML ë¦¬í¬íŠ¸ ìƒì„±
        html_content = f"""
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>ì˜¤í”ˆì†ŒìŠ¤ LLM ì„±ëŠ¥ ìµœì í™” ë¦¬í¬íŠ¸</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}
                .header {{ background: linear-gradient(90deg, #4CAF50, #45a049); color: white; padding: 20px; border-radius: 10px; }}
                .section {{ margin: 20px 0; padding: 15px; border-left: 4px solid #4CAF50; background-color: #f9f9f9; }}
                table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
                th, td {{ padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }}
                th {{ background-color: #4CAF50; color: white; }}
                .metric {{ display: inline-block; margin: 10px; padding: 15px; 
                         background: linear-gradient(135deg, #e3f2fd, #bbdefb); border-radius: 8px; 
                         box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
                .chart-container {{ margin: 20px 0; text-align: center; }}
                .highlight {{ background-color: #fff3cd; padding: 10px; border-radius: 5px; margin: 10px 0; }}
                .warning {{ background-color: #f8d7da; padding: 10px; border-radius: 5px; margin: 10px 0; }}
                .success {{ background-color: #d4edda; padding: 10px; border-radius: 5px; margin: 10px 0; }}
            </style>
        </head>
        <body>
        """

        # í—¤ë”
        html_content += f"""
        <div class="header">
            <h1>ğŸš€ ì˜¤í”ˆì†ŒìŠ¤ LLM ì„±ëŠ¥ ìµœì í™” ë¦¬í¬íŠ¸</h1>
            <p>ìƒì„± ì¼ì‹œ: {datetime.now().strftime("%Yë…„ %mì›” %dì¼ %H:%M:%S")}</p>
            <p>ì´ ìµœì í™” ì‹¤í—˜: {len(optimization_results)}ê°œ | ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸: {len(benchmark_results)}ê°œ</p>
        </div>
        """

        # ìš”ì•½ í†µê³„
        if optimization_results or benchmark_results:
            html_content += """
            <div class="section">
                <h2>ğŸ“Š ì„±ëŠ¥ ìš”ì•½</h2>
            """

            if optimization_results:
                best_opt = max(optimization_results, key=lambda x: x.get('best_score', 0))
                avg_score = np.mean([r.get('best_score', 0) for r in optimization_results])

                html_content += f"""
                <div class="metric">
                    <strong>ìµœê³  ìµœì í™” ì ìˆ˜</strong><br>
                    {best_opt.get('best_score', 0):.3f}<br>
                    <small>ëª¨ë¸: {best_opt.get('model_name', 'Unknown')}</small>
                </div>
                <div class="metric">
                    <strong>í‰ê·  ìµœì í™” ì ìˆ˜</strong><br>
                    {avg_score:.3f}<br>
                    <small>ì „ì²´ {len(optimization_results)}ê°œ ì‹¤í—˜</small>
                </div>
                """

            if benchmark_results:
                fastest = max(benchmark_results,
                            key=lambda x: x.get('performance_metrics', {}).get('tokens_per_second', 0))
                avg_speed = np.mean([r.get('performance_metrics', {}).get('tokens_per_second', 0)
                                   for r in benchmark_results])

                html_content += f"""
                <div class="metric">
                    <strong>ìµœê³  ì²˜ë¦¬ ì†ë„</strong><br>
                    {fastest.get('performance_metrics', {}).get('tokens_per_second', 0):.1f} tokens/sec<br>
                    <small>ëª¨ë¸: {fastest.get('model_name', 'Unknown')}</small>
                </div>
                <div class="metric">
                    <strong>í‰ê·  ì²˜ë¦¬ ì†ë„</strong><br>
                    {avg_speed:.1f} tokens/sec<br>
                    <small>ì „ì²´ {len(benchmark_results)}ê°œ í…ŒìŠ¤íŠ¸</small>
                </div>
                """

            html_content += "</div>"

        # ëª¨ë¸ë³„ ìƒì„¸ ë¶„ì„
        if optimization_results:
            html_content += """
            <div class="section">
                <h2>ğŸ”§ ìµœì í™” ê²°ê³¼ ìƒì„¸ ë¶„ì„</h2>
                <table>
                    <tr>
                        <th>ëª¨ë¸</th>
                        <th>ë°ì´í„°ì…‹</th>
                        <th>ìµœê³  ì ìˆ˜</th>
                        <th>ìµœì  Temperature</th>
                        <th>ìµœì  Top-p</th>
                        <th>ì†Œìš” ì‹œê°„</th>
                    </tr>
            """

            for result in sorted(optimization_results, key=lambda x: x.get('best_score', 0), reverse=True):
                best_params = result.get('best_params', {})
                html_content += f"""
                <tr>
                    <td>{result.get('model_name', 'Unknown')}</td>
                    <td>{result.get('dataset_name', 'Unknown')}</td>
                    <td>{result.get('best_score', 0):.3f}</td>
                    <td>{best_params.get('temperature', 0):.3f}</td>
                    <td>{best_params.get('top_p', 0):.3f}</td>
                    <td>{result.get('total_time', 0):.1f}ì´ˆ</td>
                </tr>
                """

            html_content += "</table></div>"

        # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        if benchmark_results:
            html_content += """
            <div class="section">
                <h2>âš¡ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„</h2>
                <table>
                    <tr>
                        <th>ëª¨ë¸</th>
                        <th>ì²˜ë¦¬ ì†ë„</th>
                        <th>ì§€ì—°ì‹œê°„ P95</th>
                        <th>ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰</th>
                        <th>1Kí† í°ë‹¹ ë¹„ìš©</th>
                        <th>íš¨ìœ¨ì„± ì ìˆ˜</th>
                    </tr>
            """

            for result in sorted(benchmark_results,
                               key=lambda x: x.get('performance_metrics', {}).get('tokens_per_second', 0),
                               reverse=True):
                perf = result.get('performance_metrics', {})
                cost = result.get('cost_analysis', {})
                eff = result.get('hardware_efficiency', {})

                html_content += f"""
                <tr>
                    <td>{result.get('model_name', 'Unknown')}</td>
                    <td>{perf.get('tokens_per_second', 0):.1f} tokens/sec</td>
                    <td>{perf.get('latency_p95', 0):.3f}ì´ˆ</td>
                    <td>{perf.get('memory_usage_mb', 0):.0f}MB</td>
                    <td>${cost.get('cost_per_1k_tokens_usd', 0):.6f}</td>
                    <td>{eff.get('overall_efficiency', 0):.3f}</td>
                </tr>
                """

            html_content += "</table></div>"

        # ìµœì  ì„¤ì • ì¶”ì²œ
        html_content += """
        <div class="section">
            <h2>ğŸ’¡ ìµœì  ì„¤ì • ì¶”ì²œ</h2>
        """

        if optimization_results:
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
            best_model = max(optimization_results, key=lambda x: x.get('best_score', 0))
            best_params = best_model.get('best_params', {})

            html_content += f"""
            <div class="success">
                <h3>ğŸ† ìµœê³  ì„±ëŠ¥ ì„¤ì •</h3>
                <p><strong>ëª¨ë¸:</strong> {best_model.get('model_name', 'Unknown')}</p>
                <p><strong>ë°ì´í„°ì…‹:</strong> {best_model.get('dataset_name', 'Unknown')}</p>
                <p><strong>ì¶”ì²œ íŒŒë¼ë¯¸í„°:</strong></p>
                <ul>
                    <li>Temperature: {best_params.get('temperature', 0):.3f}</li>
                    <li>Top-p: {best_params.get('top_p', 0):.3f}</li>
                    <li>Top-k: {best_params.get('top_k', 0)}</li>
                    <li>Max tokens: {best_params.get('max_new_tokens', 0)}</li>
                    <li>Repetition penalty: {best_params.get('repetition_penalty', 0):.3f}</li>
                </ul>
            </div>
            """

        if benchmark_results:
            # ìµœê³  íš¨ìœ¨ì„± ëª¨ë¸
            fastest_model = max(benchmark_results,
                              key=lambda x: x.get('performance_metrics', {}).get('tokens_per_second', 0))

            html_content += f"""
            <div class="highlight">
                <h3>âš¡ ìµœê³  ì†ë„ ëª¨ë¸</h3>
                <p><strong>ëª¨ë¸:</strong> {fastest_model.get('model_name', 'Unknown')}</p>
                <p><strong>ì²˜ë¦¬ ì†ë„:</strong> {fastest_model.get('performance_metrics', {}).get('tokens_per_second', 0):.1f} tokens/sec</p>
                <p><strong>ì¶”ì²œ ìš©ë„:</strong> ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬, ì‹¤ì‹œê°„ ì‘ë‹µ</p>
            </div>
            """

        # ì¼ë°˜ì ì¸ ì¶”ì²œì‚¬í•­
        html_content += """
        <div class="section">
            <h3>ğŸ¯ ì¼ë°˜ ì¶”ì²œì‚¬í•­</h3>
            <ul>
                <li><strong>ì •í™•ë„ ìš°ì„ :</strong> Temperature 0.0-0.3, Top-p 0.1-0.5 ì‚¬ìš©</li>
                <li><strong>ì°½ì˜ì„± ìš°ì„ :</strong> Temperature 0.7-1.0, Top-p 0.8-0.95 ì‚¬ìš©</li>
                <li><strong>ë©”ëª¨ë¦¬ ìµœì í™”:</strong> 4-bit ì–‘ìí™” í™œìš©, ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ</li>
                <li><strong>ì†ë„ ìµœì í™”:</strong> vLLM ì—”ì§„ ì‚¬ìš©, Flash Attention í™œìš©</li>
                <li><strong>ë¹„ìš© íš¨ìœ¨ì„±:</strong> ëª¨ë¸ í¬ê¸°ì™€ ì„±ëŠ¥ì˜ ê· í˜•ì  ì°¾ê¸°</li>
            </ul>
        </div>
        """

        # í•˜ë“œì›¨ì–´ ìµœì í™” íŒ
        html_content += """
        <div class="section">
            <h2>ğŸ’» í•˜ë“œì›¨ì–´ ìµœì í™” íŒ</h2>
            <div class="warning">
                <h3>âš ï¸ ì£¼ì˜ì‚¬í•­</h3>
                <ul>
                    <li>GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ 90%ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì£¼ì˜</li>
                    <li>ëŒ€ìš©ëŸ‰ ëª¨ë¸ì€ í…ì„œ ë³‘ë ¬í™” ê³ ë ¤</li>
                    <li>ë°°ì¹˜ í¬ê¸°ëŠ” ë©”ëª¨ë¦¬ í•œê³„ ë‚´ì—ì„œ ìµœëŒ€í•œ í¬ê²Œ ì„¤ì •</li>
                </ul>
            </div>
            
            <h3>ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ ë°©ë²•</h3>
            <ul>
                <li><strong>ì–‘ìí™”:</strong> 4-bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 75% ê°ì†Œ</li>
                <li><strong>ì»´íŒŒì¼ ìµœì í™”:</strong> torch.compile() ì‚¬ìš©ìœ¼ë¡œ 10-20% ì†ë„ í–¥ìƒ</li>
                <li><strong>ë™ì  ë°°ì¹­:</strong> vLLMì˜ ë™ì  ë°°ì¹­ìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ ì¦ëŒ€</li>
                <li><strong>Flash Attention:</strong> ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜</li>
            </ul>
        </div>
        """

        # ë§ˆë¬´ë¦¬
        html_content += """
        <div class="section">
            <h2>ğŸ“ˆ ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„</h2>
            <p>ì´ ë¦¬í¬íŠ¸ëŠ” ì˜¤í”ˆì†ŒìŠ¤ LLMì˜ ì¶”ë¡  ì„±ëŠ¥ ìµœì í™” ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•œ ê²ƒì…ë‹ˆë‹¤.</p>
            
            <h3>ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ:</h3>
            <ol>
                <li>ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¥¼ í”„ë¡œë•ì…˜ í™˜ê²½ì— ì ìš©</li>
                <li>ì •ê¸°ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰</li>
                <li>ìƒˆë¡œìš´ ëª¨ë¸ì´ë‚˜ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì¶”ê°€ ìµœì í™”</li>
                <li>í•˜ë“œì›¨ì–´ ì—…ê·¸ë ˆì´ë“œ ì‹œ ì¬ìµœì í™” ìˆ˜í–‰</li>
            </ol>
            
            <p><small>ë¦¬í¬íŠ¸ ìƒì„± ì‹œìŠ¤í…œ: ì˜¤í”ˆì†ŒìŠ¤ LLM ì¶”ë¡  ì„±ëŠ¥ ìµœì í™” ë„êµ¬</small></p>
        </div>
        
        </body>
        </html>
        """

        # íŒŒì¼ ì €ì¥
        report_path = self.viz_dir / save_path
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_path}")
        return report_path

class PerformanceAnalyzer:
    """ì„±ëŠ¥ ë¶„ì„ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤"""

    @staticmethod
    def analyze_optimization_trends(results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ìµœì í™” íŠ¸ë Œë“œ ë¶„ì„"""
        if not results:
            return {}

        # ì‹œê°„ìˆœ ì •ë ¬
        sorted_results = sorted(results, key=lambda x: x.get('timestamp', ''))

        scores = [r.get('best_score', 0) for r in sorted_results]

        analysis = {
            'total_experiments': len(results),
            'score_trend': 'improving' if scores[-1] > scores[0] else 'declining',
            'best_score': max(scores),
            'worst_score': min(scores),
            'average_score': np.mean(scores),
            'score_variance': np.var(scores),
            'improvement_rate': (scores[-1] - scores[0]) / len(scores) if len(scores) > 1 else 0
        }

        return analysis

    @staticmethod
    def find_optimal_parameters(results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ìµœì  íŒŒë¼ë¯¸í„° íŒ¨í„´ ì°¾ê¸°"""
        if not results:
            return {}

        # íŒŒë¼ë¯¸í„°ë³„ ì„±ëŠ¥ ë¶„ì„
        param_performance = {
            'temperature': {},
            'top_p': {},
            'top_k': {},
            'max_new_tokens': {},
            'repetition_penalty': {}
        }

        for result in results:
            score = result.get('best_score', 0)
            params = result.get('best_params', {})

            for param_name in param_performance.keys():
                param_value = params.get(param_name, 0)

                # ê°’ì„ êµ¬ê°„ìœ¼ë¡œ ê·¸ë£¹í™”
                if param_name == 'temperature':
                    bucket = round(param_value, 1)
                elif param_name == 'top_p':
                    bucket = round(param_value, 1)
                elif param_name == 'top_k':
                    bucket = int(param_value // 10) * 10  # 10ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
                elif param_name == 'max_new_tokens':
                    bucket = int(param_value // 100) * 100  # 100ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
                elif param_name == 'repetition_penalty':
                    bucket = round(param_value, 2)

                if bucket not in param_performance[param_name]:
                    param_performance[param_name][bucket] = []
                param_performance[param_name][bucket].append(score)

        # ê° íŒŒë¼ë¯¸í„°ì˜ ìµœì ê°’ ì°¾ê¸°
        optimal_params = {}
        for param_name, buckets in param_performance.items():
            if buckets:
                # ê° êµ¬ê°„ì˜ í‰ê·  ì„±ëŠ¥ ê³„ì‚°
                bucket_averages = {
                    bucket: np.mean(scores)
                    for bucket, scores in buckets.items()
                }

                # ìµœê³  ì„±ëŠ¥ êµ¬ê°„ ì°¾ê¸°
                best_bucket = max(bucket_averages, key=bucket_averages.get)
                optimal_params[param_name] = {
                    'value': best_bucket,
                    'performance': bucket_averages[best_bucket],
                    'sample_count': len(buckets[best_bucket])
                }

        return optimal_params

    @staticmethod
    def calculate_efficiency_score(benchmark_result: Dict[str, Any]) -> float:
        """ì¢…í•© íš¨ìœ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        perf = benchmark_result.get('performance_metrics', {})
        cost = benchmark_result.get('cost_analysis', {})

        # ì •ê·œí™”ëœ ì ìˆ˜ë“¤
        speed_score = min(perf.get('tokens_per_second', 0) / 100, 1.0)
        memory_score = max(0, 1.0 - (perf.get('memory_usage_mb', 0) / 10000))
        cost_score = max(0, 1.0 - (cost.get('cost_per_1k_tokens_usd', 0) * 10000))
        latency_score = max(0, 1.0 - perf.get('latency_p95', 0))

        # ê°€ì¤‘ í‰ê· 
        efficiency_score = (
            speed_score * 0.3 +
            memory_score * 0.25 +
            cost_score * 0.25 +
            latency_score * 0.2
        )

        return efficiency_score

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì‹œê°í™” ê°ì²´ ìƒì„±
    visualizer = ResultVisualizer()

    print("=== ì˜¤í”ˆì†ŒìŠ¤ LLM ìµœì í™” ì‹œê°í™” ì‹œìŠ¤í…œ ===")

    # ë”ë¯¸ ìµœì í™” ê²°ê³¼ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
    dummy_optimization = [
        {
            'model_name': 'llama2-7b',
            'dataset_name': 'korean_math',
            'best_score': 0.85,
            'best_params': {
                'temperature': 0.1,
                'top_p': 0.3,
                'top_k': 10,
                'max_new_tokens': 200,
                'repetition_penalty': 1.05
            },
            'total_time': 1200,
            'hardware_usage': {
                'cpu_avg': 45.2,
                'memory_peak': 78.5,
                'gpu_memory_peak': 12.3
            },
            'timestamp': '2024-01-15T10:30:00'
        },
        {
            'model_name': 'mistral-7b',
            'dataset_name': 'korean_qa',
            'best_score': 0.82,
            'best_params': {
                'temperature': 0.2,
                'top_p': 0.5,
                'top_k': 20,
                'max_new_tokens': 300,
                'repetition_penalty': 1.1
            },
            'total_time': 980,
            'hardware_usage': {
                'cpu_avg': 52.1,
                'memory_peak': 82.3,
                'gpu_memory_peak': 10.8
            },
            'timestamp': '2024-01-15T11:45:00'
        }
    ]

    # ë”ë¯¸ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìƒì„±
    dummy_benchmark = [
        {
            'model_name': 'llama2-7b',
            'dataset_name': 'korean_math',
            'performance_metrics': {
                'tokens_per_second': 75.5,
                'latency_p50': 0.8,
                'latency_p95': 1.2,
                'latency_p99': 1.8,
                'memory_usage_mb': 6400,
                'throughput': 1.25
            },
            'cost_analysis': {
                'cost_per_hour_usd': 0.15,
                'cost_per_1k_tokens_usd': 0.000025
            },
            'hardware_efficiency': {
                'tokens_per_watt': 0.755,
                'memory_efficiency': 0.0118,
                'overall_efficiency': 0.342
            },
            'timestamp': '2024-01-15T12:00:00'
        }
    ]

    try:
        # 1. ìµœì í™” ë¶„ì„ ì°¨íŠ¸
        if dummy_optimization:
            print("1. ìµœì í™” ë¶„ì„ ì°¨íŠ¸ ìƒì„±...")
            fig1 = visualizer.create_optimization_analysis(dummy_optimization, "test_optimization")
            print("   âœ… ì™„ë£Œ: test_optimization.html")

        # 2. ë²¤ì¹˜ë§ˆí¬ ë¶„ì„ ì°¨íŠ¸
        if dummy_benchmark:
            print("2. ë²¤ì¹˜ë§ˆí¬ ë¶„ì„ ì°¨íŠ¸ ìƒì„±...")
            fig2 = visualizer.create_benchmark_analysis(dummy_benchmark, "test_benchmark")
            print("   âœ… ì™„ë£Œ: test_benchmark.html")

        # 3. ëª¨ë¸ ë¹„êµ ì°¨íŠ¸
        print("3. ëª¨ë¸ ë¹„êµ ì°¨íŠ¸ ìƒì„±...")
        all_results = dummy_optimization + dummy_benchmark
        fig3 = visualizer.create_model_comparison_chart(all_results, "test_comparison")
        print("   âœ… ì™„ë£Œ: test_comparison.html")

        # 4. í•˜ë“œì›¨ì–´ ë¶„ì„
        print("4. í•˜ë“œì›¨ì–´ ë¶„ì„ ì°¨íŠ¸ ìƒì„±...")
        from config import HardwareDetector
        hardware_info = HardwareDetector.detect_hardware()
        fig4 = visualizer.create_hardware_analysis(hardware_info, "test_hardware")
        print("   âœ… ì™„ë£Œ: test_hardware.html")

        # 5. ì¢…í•© ëŒ€ì‹œë³´ë“œ
        print("5. ì¢…í•© ëŒ€ì‹œë³´ë“œ ìƒì„±...")
        fig5 = visualizer.create_comprehensive_dashboard("test_dashboard")
        print("   âœ… ì™„ë£Œ: test_dashboard.html")

        # 6. ì„±ëŠ¥ ë¦¬í¬íŠ¸
        print("6. ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±...")
        report_path = visualizer.generate_performance_report("test_report.html")
        print(f"   âœ… ì™„ë£Œ: {report_path}")

        # 7. ì„±ëŠ¥ ë¶„ì„
        print("7. ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰...")
        analyzer = PerformanceAnalyzer()

        trends = analyzer.analyze_optimization_trends(dummy_optimization)
        print(f"   ìµœì í™” íŠ¸ë Œë“œ: {trends.get('score_trend', 'unknown')}")
        print(f"   í‰ê·  ì ìˆ˜: {trends.get('average_score', 0):.3f}")

        optimal_params = analyzer.find_optimal_parameters(dummy_optimization)
        print(f"   ìµœì  Temperature: {optimal_params.get('temperature', {}).get('value', 'N/A')}")

        if dummy_benchmark:
            efficiency = analyzer.calculate_efficiency_score(dummy_benchmark[0])
            print(f"   íš¨ìœ¨ì„± ì ìˆ˜: {efficiency:.3f}")

        print(f"\nğŸ“ ëª¨ë“  ì‹œê°í™” íŒŒì¼ì´ 'visualizations' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸŒ HTML íŒŒì¼ì„ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    except Exception as e:
        print(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()