"""
ì•ˆì „ì„± ê°•í™”ëœ ì˜¤í”ˆì†ŒìŠ¤ LLM ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤
ë©”ëª¨ë¦¬ ëˆ„ìˆ˜, ìŠ¤ë ˆë“œ ì•ˆì „ì„±, ì„±ëŠ¥ ë¬¸ì œ í•´ê²°
"""
import asyncio
import time
import torch
import psutil
import gc
import threading
import uuid
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, AsyncIterator
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
from contextlib import asynccontextmanager, contextmanager
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue
import weakref

from config import ModelConfig, InferenceParams, OptimizationConfig, get_resource_manager

@dataclass
class ModelResponse:
    """ëª¨ë¸ ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    model: str
    generation_stats: Dict[str, Any]
    latency: float
    timestamp: datetime
    params: InferenceParams
    metadata: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

@dataclass
class BatchResponse:
    """ë°°ì¹˜ ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    responses: List[ModelResponse]
    total_tokens: int
    avg_tokens_per_second: float
    peak_memory_usage: float
    total_latency: float
    throughput: float
    success_count: int
    error_count: int
    metadata: Optional[Dict[str, Any]] = None

class SafeModelInterface(ABC):
    """ì•ˆì „í•œ ê¸°ë³¸ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤"""

    def __init__(self, config: ModelConfig, optimization_config: OptimizationConfig):
        self.config = config
        self.optimization_config = optimization_config
        self.logger = logging.getLogger(f"{self.__class__.__name__}_{config.name}")

        # ëª¨ë¸ ìƒíƒœ
        self.model = None
        self.tokenizer = None
        self.device = None
        self._is_loaded = False
        self._load_lock = threading.RLock()

        # ì•ˆì „ì„± ê´€ë ¨
        self._memory_monitor = MemoryMonitor()
        self._request_limiter = RequestLimiter(max_concurrent=4)
        self._health_checker = HealthChecker()

        # ë¦¬ì†ŒìŠ¤ ë§¤ë‹ˆì €ì— ë“±ë¡
        self.resource_manager = get_resource_manager()
        self.resource_manager.register_model(config.name, self)

        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_model()

    @abstractmethod
    def _load_model_impl(self):
        """ì‹¤ì œ ëª¨ë¸ ë¡œë”© êµ¬í˜„ (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass

    def _initialize_model(self):
        """ì•ˆì „í•œ ëª¨ë¸ ì´ˆê¸°í™”"""
        with self._load_lock:
            if self._is_loaded:
                return

            try:
                # ë©”ëª¨ë¦¬ ì²´í¬
                if not self._memory_monitor.check_available_memory(self.config):
                    raise RuntimeError("Insufficient memory for model loading")

                # í—¬ìŠ¤ ì²´í¬
                if not self._health_checker.check_system_health():
                    raise RuntimeError("System health check failed")

                # ì‹¤ì œ ëª¨ë¸ ë¡œë”©
                self._load_model_impl()
                self._is_loaded = True

                self.logger.info(f"Model {self.config.name} loaded successfully")

            except Exception as e:
                self.logger.error(f"Failed to initialize model {self.config.name}: {e}")
                self._cleanup()
                raise

    async def generate_async(self, prompt: str, params: InferenceParams) -> ModelResponse:
        """ì•ˆì „í•œ ë¹„ë™ê¸° í…ìŠ¤íŠ¸ ìƒì„±"""
        # ìš”ì²­ ì œí•œ í™•ì¸
        async with self._request_limiter:
            return await self._generate_with_safety_checks(prompt, params)

    async def _generate_with_safety_checks(self, prompt: str, params: InferenceParams) -> ModelResponse:
        """ì•ˆì „ì„± ê²€ì‚¬ë¥¼ í¬í•¨í•œ ìƒì„±"""
        start_time = time.time()

        try:
            # ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸
            if not self._is_loaded:
                self._initialize_model()

            # ë©”ëª¨ë¦¬ ì²´í¬
            self._memory_monitor.check_memory_before_generation()

            # ì…ë ¥ ê²€ì¦
            validated_prompt = self._validate_input(prompt)
            validated_params = self._validate_params(params)

            # ì‹¤ì œ ìƒì„±
            response = await self._generate_impl(validated_prompt, validated_params)

            # í›„ì²˜ë¦¬
            response.latency = time.time() - start_time
            response.timestamp = datetime.now()
            response.model = self.config.name

            return response

        except Exception as e:
            self.logger.error(f"Generation failed: {e}")
            return ModelResponse(
                content="",
                model=self.config.name,
                generation_stats={},
                latency=time.time() - start_time,
                timestamp=datetime.now(),
                params=params,
                error=str(e)
            )

    @abstractmethod
    async def _generate_impl(self, prompt: str, params: InferenceParams) -> ModelResponse:
        """ì‹¤ì œ ìƒì„± êµ¬í˜„ (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass

    def _validate_input(self, prompt: str) -> str:
        """ì…ë ¥ ê²€ì¦"""
        if not prompt or not prompt.strip():
            raise ValueError("Empty prompt")

        # ê¸¸ì´ ì œí•œ
        if len(prompt) > 100000:  # 100K ë¬¸ì ì œí•œ
            self.logger.warning(f"Prompt too long ({len(prompt)} chars), truncating")
            prompt = prompt[:100000]

        return prompt.strip()

    def _validate_params(self, params: InferenceParams) -> InferenceParams:
        """íŒŒë¼ë¯¸í„° ê²€ì¦"""
        # ì´ë¯¸ __post_init__ì—ì„œ ê²€ì¦ë˜ì§€ë§Œ ì¶”ê°€ ê²€ì¦
        if params.max_new_tokens > 4096:
            self.logger.warning(f"max_new_tokens too large ({params.max_new_tokens}), limiting to 4096")
            params.max_new_tokens = 4096

        return params

    async def generate_batch_async(self, prompts: List[str], params: InferenceParams,
                                  max_concurrent: int = None) -> BatchResponse:
        """ì•ˆì „í•œ ë°°ì¹˜ ìƒì„±"""
        if not prompts:
            return BatchResponse([], 0, 0, 0, 0, 0, 0, 0)

        max_concurrent = max_concurrent or min(len(prompts), self._request_limiter.max_concurrent)
        start_time = time.time()

        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ
        semaphore = asyncio.Semaphore(max_concurrent)

        async def generate_with_semaphore(prompt: str) -> ModelResponse:
            async with semaphore:
                return await self.generate_async(prompt, params)

        # ë°°ì¹˜ ì²˜ë¦¬
        tasks = [generate_with_semaphore(prompt) for prompt in prompts]
        responses = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì²˜ë¦¬
        valid_responses = []
        error_count = 0

        for response in responses:
            if isinstance(response, Exception):
                error_count += 1
                self.logger.error(f"Batch generation error: {response}")
            elif isinstance(response, ModelResponse):
                if response.error:
                    error_count += 1
                else:
                    valid_responses.append(response)

        # í†µê³„ ê³„ì‚°
        total_latency = time.time() - start_time
        total_tokens = sum(r.generation_stats.get('total_tokens', 0) for r in valid_responses)
        avg_tokens_per_second = total_tokens / total_latency if total_latency > 0 else 0
        throughput = len(valid_responses) / total_latency if total_latency > 0 else 0
        peak_memory = self._memory_monitor.get_peak_memory_usage()

        return BatchResponse(
            responses=valid_responses,
            total_tokens=total_tokens,
            avg_tokens_per_second=avg_tokens_per_second,
            peak_memory_usage=peak_memory,
            total_latency=total_latency,
            throughput=throughput,
            success_count=len(valid_responses),
            error_count=error_count
        )

    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        try:
            info = {
                'name': self.config.name,
                'path': self.config.model_path,
                'type': self.config.model_type,
                'device': str(self.device) if self.device else 'unknown',
                'loaded': self._is_loaded,
                'memory_usage_mb': self._get_memory_usage(),
                'health_status': self._health_checker.get_status()
            }

            if self._is_loaded and self.model:
                try:
                    param_count = sum(p.numel() for p in self.model.parameters())
                    info['parameter_count'] = param_count
                    info['parameter_count_human'] = self._format_parameter_count(param_count)
                except:
                    pass

            return info

        except Exception as e:
            self.logger.error(f"Error getting model info: {e}")
            return {'name': self.config.name, 'error': str(e)}

    def _format_parameter_count(self, count: int) -> str:
        """íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì½ê¸° ì‰½ê²Œ í¬ë§·"""
        if count >= 1e9:
            return f"{count/1e9:.1f}B"
        elif count >= 1e6:
            return f"{count/1e6:.1f}M"
        elif count >= 1e3:
            return f"{count/1e3:.1f}K"
        else:
            return str(count)

    def _get_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (MB)"""
        return self._memory_monitor.get_current_memory_usage()

    def _cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self, 'model') and self.model is not None:
                # ëª¨ë¸ì„ CPUë¡œ ì´ë™
                if hasattr(self.model, 'cpu'):
                    self.model.cpu()

                # ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
                del self.model
                self.model = None

            if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                del self.tokenizer
                self.tokenizer = None

            # CUDA ìºì‹œ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            gc.collect()

            self._is_loaded = False
            self.logger.info(f"Model {self.config.name} cleaned up")

        except Exception as e:
            self.logger.error(f"Error during cleanup: {e}")

    def __del__(self):
        """ì†Œë©¸ì"""
        self._cleanup()

class SafeTransformersInterface(SafeModelInterface):
    """ì•ˆì „í•œ Transformers ì¸í„°í˜ì´ìŠ¤"""

    def _load_model_impl(self):
        """Transformers ëª¨ë¸ ë¡œë”©"""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

            self.logger.info(f"Loading Transformers model: {self.config.model_path}")

            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_path,
                trust_remote_code=self.config.trust_remote_code,
                padding_side="left",
                use_fast=True  # fast tokenizer ì‚¬ìš©
            )

            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # ì–‘ìí™” ì„¤ì •
            quantization_config = None
            if self.config.load_in_4bit:
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
            elif self.config.load_in_8bit:
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)

            # ì¥ì¹˜ ë§µ ì„¤ì •
            device_map = self.config.device
            if device_map == "auto":
                device_map = "auto"
            elif device_map == "cpu":
                device_map = "cpu"
            else:
                device_map = {"": self.config.device}

            # ëª¨ë¸ ë¡œë“œ arguments
            model_kwargs = {
                'trust_remote_code': self.config.trust_remote_code,
                'torch_dtype': getattr(torch, self.config.dtype),
                'device_map': device_map,
                'low_cpu_mem_usage': True,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
            }

            if quantization_config:
                model_kwargs['quantization_config'] = quantization_config

            # Flash Attention ì„¤ì •
            if self.config.use_flash_attention:
                try:
                    model_kwargs['attn_implementation'] = "flash_attention_2"
                except:
                    self.logger.warning("Flash Attention 2 not available, using default")

            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                **model_kwargs
            )

            # ìµœì í™” ì ìš©
            self._apply_optimizations()

            # ì¥ì¹˜ ì •ë³´ ì €ì¥
            self.device = next(self.model.parameters()).device
            self.logger.info(f"Model loaded on device: {self.device}")

        except Exception as e:
            self.logger.error(f"Failed to load Transformers model: {e}")
            raise

    def _apply_optimizations(self):
        """ëª¨ë¸ ìµœì í™” ì ìš©"""
        try:
            # BetterTransformer (ì•ˆì „í•˜ê²Œ ì ìš©)
            if self.optimization_config.enable_bettertransformer:
                try:
                    self.model = self.model.to_bettertransformer()
                    self.logger.info("BetterTransformer enabled")
                except Exception as e:
                    self.logger.warning(f"BetterTransformer failed: {e}")

            # Torch Compile (ì•ˆì „í•˜ê²Œ ì ìš©)
            if self.optimization_config.enable_torch_compile:
                try:
                    # Python 3.11+ ë° ì ì ˆí•œ PyTorch ë²„ì „ì—ì„œë§Œ
                    import sys
                    if sys.version_info >= (3, 11):
                        self.model = torch.compile(self.model, mode="default")
                        self.logger.info("Torch compile enabled")
                    else:
                        self.logger.warning("Torch compile requires Python 3.11+")
                except Exception as e:
                    self.logger.warning(f"Torch compile failed: {e}")

            # Gradient checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)
            if self.optimization_config.gradient_checkpointing:
                try:
                    self.model.gradient_checkpointing_enable()
                    self.logger.info("Gradient checkpointing enabled")
                except Exception as e:
                    self.logger.warning(f"Gradient checkpointing failed: {e}")

        except Exception as e:
            self.logger.error(f"Optimization application failed: {e}")

    async def _generate_impl(self, prompt: str, params: InferenceParams) -> ModelResponse:
        """Transformers ìƒì„± êµ¬í˜„"""
        def generate_sync():
            # í† í°í™”
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=4096  # ì•ˆì „í•œ ìµœëŒ€ ê¸¸ì´
            ).to(self.device)

            input_length = inputs['input_ids'].shape[1]

            # ìƒì„± ì„¤ì •
            generation_kwargs = {
                'max_new_tokens': params.max_new_tokens,
                'min_new_tokens': params.min_new_tokens,
                'temperature': params.temperature,
                'top_p': params.top_p,
                'top_k': params.top_k,
                'repetition_penalty': params.repetition_penalty,
                'length_penalty': params.length_penalty,
                'do_sample': params.do_sample,
                'num_beams': params.num_beams,
                'early_stopping': params.early_stopping,
                'use_cache': params.use_cache,
                'pad_token_id': params.pad_token_id or self.tokenizer.pad_token_id,
                'eos_token_id': params.eos_token_id or self.tokenizer.eos_token_id,
                'num_return_sequences': params.num_return_sequences,
            }

            # ìƒì„± ì‹¤í–‰
            generation_start = time.time()
            with torch.no_grad():
                outputs = self.model.generate(**inputs, **generation_kwargs)
            generation_time = time.time() - generation_start

            # ê²°ê³¼ ë””ì½”ë”©
            generated_tokens = outputs[0][input_length:]
            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)

            # í†µê³„ ê³„ì‚°
            total_tokens = len(generated_tokens)
            tokens_per_second = total_tokens / generation_time if generation_time > 0 else 0

            return ModelResponse(
                content=generated_text,
                model=self.config.name,
                generation_stats={
                    'input_tokens': input_length,
                    'output_tokens': total_tokens,
                    'total_tokens': input_length + total_tokens,
                    'tokens_per_second': tokens_per_second,
                    'generation_time': generation_time,
                    'memory_usage_mb': self._get_memory_usage()
                },
                latency=0,  # ë‚˜ì¤‘ì— ì„¤ì •ë¨
                timestamp=datetime.now(),
                params=params
            )

        # ìŠ¤ë ˆë“œí’€ì—ì„œ ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=1) as executor:
            return await loop.run_in_executor(executor, generate_sync)

class MemoryMonitor:
    """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""

    def __init__(self):
        self.peak_memory = 0
        self.lock = threading.Lock()

    def check_available_memory(self, config: ModelConfig) -> bool:
        """ëª¨ë¸ ë¡œë”© ì „ ë©”ëª¨ë¦¬ ì²´í¬"""
        try:
            if config.device == "cuda" and torch.cuda.is_available():
                available_memory = torch.cuda.get_device_properties(0).total_memory
                # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ì¶”ì •
                estimated_usage = self._estimate_memory_usage(config)
                return estimated_usage < available_memory * 0.9
            return True
        except:
            return True

    def _estimate_memory_usage(self, config: ModelConfig) -> int:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (ë°”ì´íŠ¸)"""
        # ëª¨ë¸ í¬ê¸° ì¶”ì • (ë§¤ìš° ê°„ë‹¨í•œ ë²„ì „)
        size_estimates = {
            '7b': 14 * 1024**3,   # 14GB
            '13b': 26 * 1024**3,  # 26GB
            '30b': 60 * 1024**3,  # 60GB
            '70b': 140 * 1024**3, # 140GB
        }

        base_size = 14 * 1024**3  # ê¸°ë³¸ 7B í¬ê¸°
        path_lower = config.model_path.lower()

        for size, memory in size_estimates.items():
            if size in path_lower:
                base_size = memory
                break

        # ì–‘ìí™” ì ìš©
        if config.load_in_4bit:
            return int(base_size * 0.5)
        elif config.load_in_8bit:
            return int(base_size * 0.75)
        else:
            return base_size

    def check_memory_before_generation(self):
        """ìƒì„± ì „ ë©”ëª¨ë¦¬ ì²´í¬"""
        try:
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated()
                reserved = torch.cuda.memory_reserved()
                total = torch.cuda.get_device_properties(0).total_memory

                if allocated / total > 0.95:
                    torch.cuda.empty_cache()

                if reserved / total > 0.98:
                    raise RuntimeError("GPU memory usage too high")
        except Exception as e:
            logging.warning(f"Memory check failed: {e}")

    def get_current_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        try:
            if torch.cuda.is_available():
                return torch.cuda.memory_allocated() / 1024**2
            else:
                process = psutil.Process()
                return process.memory_info().rss / 1024**2
        except:
            return 0.0

    def get_peak_memory_usage(self) -> float:
        """í”¼í¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸ ë° ë°˜í™˜"""
        current = self.get_current_memory_usage()
        with self.lock:
            self.peak_memory = max(self.peak_memory, current)
            return self.peak_memory

class RequestLimiter:
    """ìš”ì²­ ì œí•œê¸°"""

    def __init__(self, max_concurrent: int = 4):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)

    async def __aenter__(self):
        await self.semaphore.acquire()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        self.semaphore.release()

class HealthChecker:
    """ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬"""

    def __init__(self):
        self.last_check = 0
        self.check_interval = 60  # 1ë¶„
        self.status = {}

    def check_system_health(self) -> bool:
        """ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬"""
        current_time = time.time()

        # ìºì‹œëœ ê²°ê³¼ ì‚¬ìš© (1ë¶„ ì´ë‚´)
        if current_time - self.last_check < self.check_interval:
            return self.status.get('healthy', True)

        try:
            # CPU ì‚¬ìš©ë¥  ì²´í¬
            cpu_percent = psutil.cpu_percent(interval=1)

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì²´í¬
            memory = psutil.virtual_memory()

            # ë””ìŠ¤í¬ ì‚¬ìš©ë¥  ì²´í¬
            disk = psutil.disk_usage('/')

            # ê±´ê°•ì„± íŒë‹¨
            healthy = (
                cpu_percent < 90 and
                memory.percent < 95 and
                disk.percent < 95
            )

            self.status = {
                'healthy': healthy,
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'disk_percent': disk.percent,
                'last_check': current_time
            }

            self.last_check = current_time
            return healthy

        except Exception as e:
            logging.warning(f"Health check failed: {e}")
            return True  # ì²´í¬ ì‹¤íŒ¨ ì‹œ í†µê³¼ë¡œ ì²˜ë¦¬

    def get_status(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ ìƒíƒœ ë°˜í™˜"""
        return self.status.copy()

class SafeModelManager:
    """ì•ˆì „í•œ ëª¨ë¸ ë§¤ë‹ˆì €"""

    def __init__(self, optimization_config: OptimizationConfig):
        self.optimization_config = optimization_config
        self.models: Dict[str, SafeModelInterface] = {}
        self.model_locks: Dict[str, threading.RLock] = {}
        self.logger = logging.getLogger(__name__)

        # ì „ì—­ ë½
        self._global_lock = threading.RLock()

    def register_model(self, name: str, config: ModelConfig, force_reload: bool = False):
        """ì•ˆì „í•œ ëª¨ë¸ ë“±ë¡"""
        with self._global_lock:
            # ê¸°ì¡´ ëª¨ë¸ ì •ë¦¬ (í•„ìš”í•œ ê²½ìš°)
            if name in self.models and force_reload:
                self.unregister_model(name)

            if name in self.models:
                self.logger.info(f"Model {name} already registered")
                return

            # ëª¨ë¸ë³„ ë½ ìƒì„±
            self.model_locks[name] = threading.RLock()

            try:
                # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì¸í„°í˜ì´ìŠ¤ ì„ íƒ
                if config.model_type == "transformers":
                    interface = SafeTransformersInterface(config, self.optimization_config)
                else:
                    # ë‹¤ë¥¸ íƒ€ì…ë“¤ë„ SafeTransformersInterfaceë¡œ ì²˜ë¦¬ (ì¼ë‹¨)
                    # ì¶”í›„ vLLM, Ollama ë“±ì˜ ì•ˆì „í•œ êµ¬í˜„ ì¶”ê°€
                    interface = SafeTransformersInterface(config, self.optimization_config)

                self.models[name] = interface
                self.logger.info(f"Model {name} registered successfully")

            except Exception as e:
                self.logger.error(f"Failed to register model {name}: {e}")
                # ì‹¤íŒ¨ ì‹œ ì •ë¦¬
                if name in self.model_locks:
                    del self.model_locks[name]
                raise

    def unregister_model(self, name: str):
        """ëª¨ë¸ ë“±ë¡ í•´ì œ"""
        with self._global_lock:
            if name not in self.models:
                return

            with self.model_locks.get(name, threading.RLock()):
                try:
                    # ëª¨ë¸ ì •ë¦¬
                    model = self.models[name]
                    model._cleanup()

                    # ë”•ì…”ë„ˆë¦¬ì—ì„œ ì œê±°
                    del self.models[name]
                    if name in self.model_locks:
                        del self.model_locks[name]

                    self.logger.info(f"Model {name} unregistered")

                except Exception as e:
                    self.logger.error(f"Error unregistering model {name}: {e}")

    def get_model(self, name: str) -> Optional[SafeModelInterface]:
        """ì•ˆì „í•œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ ë°˜í™˜"""
        with self._global_lock:
            if name not in self.models:
                self.logger.warning(f"Model {name} not found")
                return None

            return self.models[name]

    def list_models(self) -> List[str]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        with self._global_lock:
            return list(self.models.keys())

    def get_models_info(self) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  ëª¨ë¸ ì •ë³´"""
        with self._global_lock:
            return {
                name: model.get_model_info()
                for name, model in self.models.items()
            }

    async def generate_batch_all_models(self, prompts: List[str],
                                       params: InferenceParams) -> Dict[str, BatchResponse]:
        """ëª¨ë“  ëª¨ë¸ì—ì„œ ë°°ì¹˜ ìƒì„± (ë¹„êµìš©)"""
        results = {}

        for name, model in self.models.items():
            try:
                self.logger.info(f"Running batch generation on {name}")
                result = await model.generate_batch_async(prompts, params)
                results[name] = result
            except Exception as e:
                self.logger.error(f"Batch generation failed for {name}: {e}")
                results[name] = BatchResponse([], 0, 0, 0, 0, 0, 0, len(prompts))

        return results

    def cleanup_all(self):
        """ëª¨ë“  ëª¨ë¸ ì •ë¦¬"""
        with self._global_lock:
            model_names = list(self.models.keys())
            for name in model_names:
                self.unregister_model(name)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    import asyncio
    from config import ModelConfig, OptimizationConfig, InferenceParams

    async def test_safe_interface():
        print("=== ì•ˆì „í•œ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ===")

        # ì„¤ì •
        model_config = ModelConfig(
            name="test-model",
            model_path="microsoft/DialoGPT-medium",  # ì‘ì€ í…ŒìŠ¤íŠ¸ ëª¨ë¸
            model_type="transformers",
            device="auto",
            dtype="float16",
            load_in_4bit=True,
            trust_remote_code=False
        )

        optimization_config = OptimizationConfig(
            enable_torch_compile=False,
            enable_bettertransformer=False,
            auto_cleanup=True
        )

        try:
            # ëª¨ë¸ ë§¤ë‹ˆì € ìƒì„±
            manager = SafeModelManager(optimization_config)

            # ëª¨ë¸ ë“±ë¡
            manager.register_model("test", model_config)
            print("âœ… ëª¨ë¸ ë“±ë¡ ì™„ë£Œ")

            # ëª¨ë¸ ì •ë³´ í™•ì¸
            model = manager.get_model("test")
            if model:
                info = model.get_model_info()
                print(f"ğŸ“‹ ëª¨ë¸ ì •ë³´: {info}")

                # ë‹¨ì¼ ìƒì„± í…ŒìŠ¤íŠ¸
                params = InferenceParams(
                    temperature=0.7,
                    max_new_tokens=50,
                    top_p=0.9
                )

                response = await model.generate_async("ì•ˆë…•í•˜ì„¸ìš”!", params)
                print(f"ğŸ”¤ ìƒì„± ê²°ê³¼: {response.content}")
                print(f"â±ï¸ ì§€ì—°ì‹œê°„: {response.latency:.2f}ì´ˆ")

                # ë°°ì¹˜ ìƒì„± í…ŒìŠ¤íŠ¸
                prompts = ["ì•ˆë…•í•˜ì„¸ìš”!", "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”?", "AIì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."]
                batch_response = await model.generate_batch_async(prompts, params)
                print(f"ğŸ“¦ ë°°ì¹˜ ê²°ê³¼: {batch_response.success_count}ê°œ ì„±ê³µ, {batch_response.error_count}ê°œ ì‹¤íŒ¨")
                print(f"ğŸš€ ì²˜ë¦¬ ì†ë„: {batch_response.avg_tokens_per_second:.1f} tokens/sec")

            print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # ì •ë¦¬
            if 'manager' in locals():
                manager.cleanup_all()

            # ì „ì—­ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            from config import cleanup_resources
            cleanup_resources()

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_safe_interface())