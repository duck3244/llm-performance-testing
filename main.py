else:
print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
except Exception as e:
print(f"âš ï¸ GPU ìƒíƒœ í™•ì¸ ë¶ˆê°€: {e}")

# ì˜¤ë¥˜ ì¶œë ¥
if issues:
    print(f"âŒ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ë¯¸ì¶©ì¡±:")
    for issue in issues:
        print(f"   - {issue}")

    print(f"\nğŸ’¡ í•´ê²° ë°©ë²•:")
    if missing_packages:
        print(f"   pip install {' '.join([p.split(' ')[0] for p in missing_packages])}")

return requirements_met


def show_system_status_enhanced():
    """ê°•í™”ëœ ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ"""
    print("ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸:")

    # ì„¤ì • íŒŒì¼ í™•ì¸
    config_file = Path("llm_config.json")
    if config_file.exists():
        try:
            config_manager = RobustConfigManager()
            model_count = len(config_manager.model_configs)
            print(f"   âœ… ì„¤ì • íŒŒì¼ ì¡´ì¬ ({model_count}ê°œ ëª¨ë¸ ì„¤ì •)")

            # ì„¤ì • ê²€ì¦
            validator = ConfigValidator()
            total_issues = 0
            for name, config in config_manager.model_configs.items():
                issues = validator.validate_model_config(config)
                if issues:
                    total_issues += len(issues)
                    print(f"   âš ï¸ ëª¨ë¸ '{name}': {len(issues)}ê°œ ë¬¸ì œ")

            if total_issues == 0:
                print("   âœ… ëª¨ë“  ëª¨ë¸ ì„¤ì •ì´ ìœ íš¨í•©ë‹ˆë‹¤")

        except Exception as e:
            print(f"   âŒ ì„¤ì • íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    else:
        print("   âŒ ì„¤ì • íŒŒì¼ ì—†ìŒ (python safe_main.py init ì‹¤í–‰ í•„ìš”)")

    # ë””ë ‰í† ë¦¬ í™•ì¸
    required_dirs = ["data", "optimization_results", "visualizations"]
    for dir_name in required_dirs:
        dir_path = Path(dir_name)
        if dir_path.exists():
            file_count = len(list(dir_path.glob("*")))
            print(f"   âœ… {dir_name} ë””ë ‰í† ë¦¬ ({file_count}ê°œ íŒŒì¼)")
        else:
            print(f"   âŒ {dir_name} ë””ë ‰í† ë¦¬ ì—†ìŒ")

    # í•˜ë“œì›¨ì–´ ì •ë³´ (ìƒì„¸)
    try:
        hardware_info = HardwareDetector.detect_hardware()
        print(f"   ğŸ’» ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"      CPU: {hardware_info['cpu_cores']}ì½”ì–´")
        print(f"      ë©”ëª¨ë¦¬: {hardware_info['available_memory']}/{hardware_info['total_memory']}GB")

        if hardware_info['cuda_available']:
            print(f"      GPU: {hardware_info['cuda_device_count']}ê°œ")
            for i in range(hardware_info['cuda_device_count']):
                gpu_name = hardware_info.get(f'gpu_{i}_name', f'GPU {i}')
                gpu_memory = hardware_info.get(f'gpu_{i}_memory', 0)
                print(f"        {gpu_name}: {gpu_memory}GB")
        else:
            print(f"      GPU: CUDA ì‚¬ìš© ë¶ˆê°€")

    except Exception as e:
        print(f"   âš ï¸ í•˜ë“œì›¨ì–´ ì •ë³´ í™•ì¸ ë¶ˆê°€: {e}")

    # ê²°ê³¼ íŒŒì¼ í™•ì¸ (ìƒì„¸)
    results_dir = Path("optimization_results")
    if results_dir.exists():
        opt_files = list(results_dir.glob("opt_*.json"))
        bench_files = list(results_dir.glob("bench_*.json"))
        print(f"   ğŸ“Š ì €ì¥ëœ ê²°ê³¼:")
        print(f"      ìµœì í™”: {len(opt_files)}ê°œ")
        print(f"      ë²¤ì¹˜ë§ˆí¬: {len(bench_files)}ê°œ")

        # ìµœê·¼ ê²°ê³¼ í‘œì‹œ
        if opt_files or bench_files:
            all_files = sorted(
                opt_files + bench_files,
                key=lambda x: x.stat().st_mtime,
                reverse=True
            )
            latest = all_files[0]
            mod_time = datetime.fromtimestamp(latest.stat().st_mtime)
            print(f"      ìµœì‹ : {latest.name} ({mod_time.strftime('%Y-%m-%d %H:%M')})")
    else:
        print("   ğŸ“Š ì €ì¥ëœ ê²°ê³¼: 0ê°œ")


def run_init_command(args):
    """init ëª…ë ¹ì–´ ì‹¤í–‰ - ì•ˆì „ì„± ê°•í™”"""
    print("ğŸ”§ ì•ˆì „í•œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")

    config_file = "llm_config.json"

    if Path(config_file).exists() and not args.force:
        print(f"âš ï¸  ì„¤ì • íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {config_file}")
        print("   ë®ì–´ì“°ë ¤ë©´ --force ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        return

    try:
        # ì•ˆì „í•œ ì„¤ì • ë§¤ë‹ˆì € ìƒì„±
        config_manager = RobustConfigManager(config_file)
        print(f"âœ… ì•ˆì „í•œ ì„¤ì • íŒŒì¼ ìƒì„±: {config_file}")

        # ë””ë ‰í† ë¦¬ ìƒì„±
        directories = ["data", "optimization_results", "visualizations"]
        for directory in directories:
            Path(directory).mkdir(exist_ok=True)
            print(f"   âœ… {directory} ë””ë ‰í† ë¦¬ ìƒì„±")

        # í•˜ë“œì›¨ì–´ ìë™ ê°ì§€
        if args.auto_detect:
            print("ğŸ” í•˜ë“œì›¨ì–´ ìë™ ê°ì§€ ì¤‘...")
            try:
                hardware_info = HardwareDetector.detect_hardware()
                print(f"   GPU: {hardware_info['cuda_device_count']}ê°œ")
                print(f"   ë©”ëª¨ë¦¬: {hardware_info['total_memory']}GB")

                if hardware_info['cuda_available']:
                    total_gpu_memory = sum(
                        hardware_info.get(f'gpu_{i}_memory', 0)
                        for i in range(hardware_info['cuda_device_count'])
                    )
                    print(f"   GPU ë©”ëª¨ë¦¬: {total_gpu_memory}GB")

                    # ì•ˆì „ì„± ê²€ì‚¬
                    for name, config in config_manager.model_configs.items():
                        safety_warnings = SafetyChecker.check_memory_safety(config)
                        if safety_warnings:
                            print(f"   âš ï¸ {name} ì•ˆì „ì„± ê²½ê³ :")
                            for warning in safety_warnings:
                                print(f"      - {warning}")

            except Exception as e:
                print(f"âš ï¸  í•˜ë“œì›¨ì–´ ê°ì§€ ì‹¤íŒ¨: {e}")

        print("\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. í•˜ë“œì›¨ì–´ ì •ë³´ í™•ì¸: python safe_main.py hardware")
        print("2. ëª¨ë¸ ëª©ë¡ í™•ì¸: python safe_main.py list --type models")
        print("3. ì•ˆì „í•œ ìµœì í™” ì‹¤í–‰: python safe_main.py optimize --model [ëª¨ë¸ëª…] --dataset [ë°ì´í„°ì…‹ëª…] --samples 20")
        print("4. ê²°ê³¼ í™•ì¸: python safe_main.py visualize --type dashboard")

    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        sys.exit(1)


def run_hardware_command(args):
    """hardware ëª…ë ¹ì–´ ì‹¤í–‰"""
    print("ğŸ’» í•˜ë“œì›¨ì–´ ì •ë³´ ë¶„ì„")

    try:
        hardware_info = HardwareDetector.detect_hardware()

        print(f"\nğŸ” ê°ì§€ëœ í•˜ë“œì›¨ì–´:")
        print(f"   CUDA ì‚¬ìš© ê°€ëŠ¥: {'âœ…' if hardware_info['cuda_available'] else 'âŒ'}")
        print(f"   GPU ê°œìˆ˜: {hardware_info['cuda_device_count']}")
        print(f"   ì´ ë©”ëª¨ë¦¬: {hardware_info['total_memory']}GB")
        print(f"   ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {hardware_info['available_memory']}GB")
        print(f"   CPU ì½”ì–´: {hardware_info['cpu_cores']}")

        if hardware_info['cuda_available']:
            total_gpu_memory = 0
            for i in range(hardware_info['cuda_device_count']):
                gpu_memory = hardware_info.get(f'gpu_{i}_memory', 0)
                gpu_name = hardware_info.get(f'gpu_{i}_name', 'Unknown')
                print(f"   GPU {i}: {gpu_name} ({gpu_memory}GB)")
                total_gpu_memory += gpu_memory

            print(f"\nğŸ¯ ì•ˆì „í•œ ëª¨ë¸ ì¶”ì²œ:")
            if total_gpu_memory >= 80:
                print("   âœ… 70B ëª¨ë¸ê¹Œì§€ ì‹¤í–‰ ê°€ëŠ¥ (ì–‘ìí™” ê¶Œì¥)")
            elif total_gpu_memory >= 32:
                print("   âœ… 13B ëª¨ë¸ê¹Œì§€ ì‹¤í–‰ ê°€ëŠ¥")
            elif total_gpu_memory >= 16:
                print("   âœ… 7B ëª¨ë¸ ì‹¤í–‰ ê°€ëŠ¥ (float16)")
            elif total_gpu_memory >= 8:
                print("   âš ï¸  7B ëª¨ë¸ ì‹¤í–‰ ê°€ëŠ¥ (4-bit ì–‘ìí™” í•„ìˆ˜)")
            else:
                print("   âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, CPU ì¶”ë¡  ê¶Œì¥")

        # ëª¨ë¸ í¬ê¸°ë³„ ì¶”ì²œ ì„¤ì •
        if hasattr(args, 'model_size') and args.model_size:
            print(f"\nğŸ¯ {args.model_size.upper()} ëª¨ë¸ ì•ˆì „ ì„¤ì •:")
            recommended = HardwareDetector.recommend_config(args.model_size)
            print(f"   ì¥ì¹˜: {recommended.device}")
            print(f"   ë°ì´í„° íƒ€ì…: {recommended.dtype}")
            print(f"   4-bit ì–‘ìí™”: {'âœ…' if recommended.load_in_4bit else 'âŒ'}")
            print(f"   8-bit ì–‘ìí™”: {'âœ…' if recommended.load_in_8bit else 'âŒ'}")

            # ì•ˆì „í•œ ë°°ì¹˜ í¬ê¸° ì¶”ì²œ
            available_memory = hardware_info.get('gpu_0_memory', 8)
            safe_batch = HardwareDetector.get_safe_batch_size(args.model_size, available_memory)
            print(f"   ê¶Œì¥ ë°°ì¹˜ í¬ê¸°: {safe_batch}")

    except Exception as e:
        print(f"âŒ í•˜ë“œì›¨ì–´ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")


async def run_optimize_command(args):
    """optimize ëª…ë ¹ì–´ ì‹¤í–‰ - ì•ˆì „ì„± ê°•í™”"""
    print(f"ğŸ”§ ì•ˆì „í•œ íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘: {args.model} on {args.dataset}")

    # ì•ˆì „ ëª¨ë“œ ì ìš©
    if hasattr(args, 'safe_mode') and args.safe_mode:
        max_samples = min(args.samples, 10)
        max_trials = min(args.trials, 5)
        print(f"   ğŸ›¡ï¸ ì•ˆì „ ëª¨ë“œ: ìƒ˜í”Œ {max_samples}ê°œ, ì‹œë„ {max_trials}íšŒë¡œ ì œí•œ")
    else:
        max_samples = min(args.samples, 50)  # ê¸°ë³¸ ì œí•œ
        max_trials = min(args.trials, 20)
        print(f"   ì „ëµ: {args.strategy}, ì‹œë„: {max_trials}íšŒ, ìƒ˜í”Œ: {max_samples}ê°œ")

    try:
        # ì•ˆì „í•œ ìµœì í™”ê¸° ì‚¬ìš©
        optimizer = SafePerformanceOptimizer()

        # ëª¨ë¸ ì„¤ì • ê²€ì¦
        config_manager = RobustConfigManager()
        model_config = config_manager.get_safe_model_config(args.model)

        if not model_config:
            print(f"âŒ ëª¨ë¸ {args.model} ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            print("   ëŒ€ì²´ ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤...")
            model_config = config_manager.create_fallback_config(args.model)

        # ì•ˆì „ì„± ê²€ì‚¬
        safety_warnings = SafetyChecker.check_memory_safety(model_config)
        if safety_warnings:
            print("âš ï¸ ì•ˆì „ì„± ê²½ê³ :")
            for warning in safety_warnings:
                print(f"   - {warning}")
            print("   ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ", end="")
            response = input()
            if response.lower() != 'y':
                sys.exit(1)
            else:
                args.safe_mode = True  # ê°•ì œë¡œ ì•ˆì „ ëª¨ë“œ í™œì„±í™”

    # ì•ˆì „í•œ ì‹¤í–‰ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ì²´í¬
    try:
        import torch
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i) / 1024 ** 3
                total = torch.cuda.get_device_properties(i).total_memory / 1024 ** 3
                if allocated / total > 0.8:
                    print(f"âš ï¸ GPU {i} ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤ ({allocated / total:.1%})")
                    print("   ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
                    torch.cuda.empty_cache()
    except:
        pass

    # ë³µì¡í•œ ëª…ë ¹ì–´ë“¤ ì²˜ë¦¬
    try:
        if args.command == 'optimize':
            await run_optimize_command(args)
        elif args.command == 'benchmark':
            await run_benchmark_command(args)
        elif args.command == 'compare':
            await run_compare_command(args)
        elif args.command == 'visualize':
            run_visualize_command(args)
        elif args.command == 'export':
            run_export_command(args)
        elif args.command == 'analyze':
            run_analyze_command(args)
        else:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {args.command}")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        if hasattr(args, 'debug') and args.debug:
            traceback.print_exc()
        else:
            print("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´: --debug ì˜µì…˜ ì‚¬ìš©")
    finally:
        # ì•ˆì „í•œ ì •ë¦¬
        print("\nğŸ§¹ ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘...")
        try:
            gc.collect()
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
        except:
            pass
        print("âœ… ì •ë¦¬ ì™„ë£Œ")


def run_visualize_command(args):
    """visualize ëª…ë ¹ì–´ ì‹¤í–‰ - ê¸°ë³¸ êµ¬í˜„"""
    print(f"ğŸ“Š ì‹œê°í™” ìƒì„±: {args.type}")

    try:
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ í™•ì¸
        results_dir = Path("optimization_results")
        if not results_dir.exists() or not list(results_dir.glob("*.json")):
            print("âŒ ì‹œê°í™”í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("   ë¨¼ì € ìµœì í™”ë‚˜ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return

        # ê¸°ë³¸ HTML ë¦¬í¬íŠ¸ ìƒì„±
        output_file = f"visualizations/{args.output or args.type}_report.html"
        Path("visualizations").mkdir(exist_ok=True)

        html_content = generate_basic_report(results_dir)

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"âœ… ê¸°ë³¸ ë¦¬í¬íŠ¸ ìƒì„±: {output_file}")
        print("   ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ì„œ í™•ì¸í•˜ì„¸ìš”.")

    except Exception as e:
        print(f"âŒ ì‹œê°í™” ì‹¤íŒ¨: {e}")


def run_export_command(args):
    """export ëª…ë ¹ì–´ ì‹¤í–‰"""
    print(f"ğŸ“¤ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°: {args.format} í˜•ì‹")

    try:
        results_dir = Path("optimization_results")
        if not results_dir.exists():
            print("âŒ ë‚´ë³´ë‚¼ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        all_results = []
        for result_file in results_dir.glob("*.json"):
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                flat_data = {
                    'file_name': result_file.name,
                    'test_id': data.get('test_id', ''),
                    'model_name': data.get('model_name', ''),
                    'dataset_name': data.get('dataset_name', ''),
                    'timestamp': data.get('timestamp', ''),
                    'test_type': 'optimization' if 'best_score' in data else 'benchmark'
                }

                if 'best_score' in data:
                    flat_data.update({
                        'best_score': data.get('best_score', 0),
                        'total_time': data.get('total_time', 0)
                    })
                elif 'performance_metrics' in data:
                    perf = data.get('performance_metrics', {})
                    flat_data.update({
                        'tokens_per_second': perf.get('tokens_per_second', 0),
                        'memory_usage_mb': perf.get('memory_usage_mb', 0)
                    })

                all_results.append(flat_data)

            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ {result_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        if not all_results:
            print("âŒ ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì¶œë ¥ íŒŒì¼ëª… ê²°ì •
        if args.output:
            output_path = Path(args.output)
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = Path(f"llm_results_{timestamp}.{args.format}")

        # í˜•ì‹ë³„ ë‚´ë³´ë‚´ê¸°
        if args.format == 'json':
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, indent=2, ensure_ascii=False)
        elif args.format == 'csv':
            try:
                import pandas as pd
                df = pd.DataFrame(all_results)
                df.to_csv(output_path, index=False, encoding='utf-8-sig')
            except ImportError:
                # pandas ì—†ì´ CSV ìƒì„±
                import csv
                if all_results:
                    with open(output_path, 'w', newline='', encoding='utf-8') as f:
                        writer = csv.DictWriter(f, fieldnames=all_results[0].keys())
                        writer.writeheader()
                        writer.writerows(all_results)

        print(f"âœ… ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")
        print(f"   ì´ {len(all_results)}ê°œ ê²°ê³¼ ë‚´ë³´ëƒ„")

    except Exception as e:
        print(f"âŒ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")


def run_analyze_command(args):
    """analyze ëª…ë ¹ì–´ ì‹¤í–‰"""
    print("ğŸ” ê²°ê³¼ ë¶„ì„ ì‹œì‘...")

    try:
        results_dir = Path("optimization_results")
        if not results_dir.exists():
            print("âŒ ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        optimization_results = []
        benchmark_results = []

        for result_file in results_dir.glob("*.json"):
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # í•„í„° ì ìš©
                if args.model and args.model.lower() not in data.get('model_name', '').lower():
                    continue
                if args.dataset and args.dataset.lower() not in data.get('dataset_name', '').lower():
                    continue

                if 'best_score' in data:
                    optimization_results.append(data)
                elif 'performance_metrics' in data:
                    benchmark_results.append(data)

            except Exception:
                continue

        print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ: ìµœì í™” {len(optimization_results)}ê°œ, ë²¤ì¹˜ë§ˆí¬ {len(benchmark_results)}ê°œ")

        # ìµœì í™” ê²°ê³¼ ë¶„ì„
        if optimization_results:
            print(f"\nğŸ”§ ìµœì í™” ê²°ê³¼ ë¶„ì„:")
            best_result = max(optimization_results, key=lambda x: x.get('best_score', 0))
            avg_score = sum(r.get('best_score', 0) for r in optimization_results) / len(optimization_results)

            print(f"   ìµœê³  ì ìˆ˜: {best_result.get('best_score', 0):.3f} ({best_result.get('model_name', 'Unknown')})")
            print(f"   í‰ê·  ì ìˆ˜: {avg_score:.3f}")

            best_params = best_result.get('best_params', {})
            print(f"   ìµœì  íŒŒë¼ë¯¸í„°:")
            print(f"     Temperature: {best_params.get('temperature', 0):.3f}")
            print(f"     Top-p: {best_params.get('top_p', 0):.3f}")
            print(f"     Top-k: {best_params.get('top_k', 0)}")

        # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„
        if benchmark_results:
            print(f"\nâš¡ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„:")
            fastest_model = max(benchmark_results,
                                key=lambda x: x.get('performance_metrics', {}).get('tokens_per_second', 0))

            fastest_speed = fastest_model.get('performance_metrics', {}).get('tokens_per_second', 0)
            print(f"   ìµœê³  ì†ë„: {fastest_speed:.1f} tokens/sec ({fastest_model.get('model_name', 'Unknown')})")

        # ì¶”ì²œì‚¬í•­
        print(f"\nğŸ’¡ ì•ˆì „ì„± ì¤‘ì‹¬ ì¶”ì²œì‚¬í•­:")
        print("   1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ì–‘ìí™” í™œìš©")
        print("   2. ë°°ì¹˜ í¬ê¸° ì¡°ì •ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´")
        print("   3. ì •ê¸°ì ì¸ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì²´í¬")
        print("   4. ì•ˆì „ ëª¨ë“œ ì‚¬ìš©ìœ¼ë¡œ ìœ„í—˜ ìš”ì†Œ ìµœì†Œí™”")

        # ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
        if args.report:
            try:
                report_path = generate_analysis_report(optimization_results, benchmark_results)
                print(f"\nğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±: {report_path}")
            except Exception as e:
                print(f"âš ï¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")

    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")


def generate_basic_report(results_dir: Path) -> str:
    """ê¸°ë³¸ HTML ë¦¬í¬íŠ¸ ìƒì„±"""
    results = []
    for file_path in results_dir.glob("*.json"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                results.append(data)
        except:
            continue

    html = f"""
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <title>LLM ìµœì í™” ê²°ê³¼ ë¦¬í¬íŠ¸</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; }}
            .header {{ background: #4CAF50; color: white; padding: 20px; border-radius: 5px; }}
            .result {{ margin: 10px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
            .optimization {{ background-color: #e3f2fd; }}
            .benchmark {{ background-color: #fff3e0; }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>ğŸš€ ì•ˆì „í•œ LLM ìµœì í™” ê²°ê³¼</h1>
            <p>ìƒì„± ì‹œê°„: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
            <p>ì´ ê²°ê³¼: {len(results)}ê°œ</p>
        </div>
    """

    for result in results:
        result_type = "optimization" if 'best_score' in result else "benchmark"
        css_class = result_type

        html += f"""
        <div class="result {css_class}">
            <h3>{'ğŸ”§ ìµœì í™”' if result_type == 'optimization' else 'âš¡ ë²¤ì¹˜ë§ˆí¬'}: {result.get('model_name', 'Unknown')}</h3>
            <p><strong>ë°ì´í„°ì…‹:</strong> {result.get('dataset_name', 'Unknown')}</p>
            <p><strong>ì‹œê°„:</strong> {result.get('timestamp', '')}</p>
        """

        if result_type == 'optimization':
            html += f"""
            <p><strong>ìµœê³  ì ìˆ˜:</strong> {result.get('best_score', 0):.3f}</p>
            <p><strong>ì†Œìš” ì‹œê°„:</strong> {result.get('total_time', 0):.1f}ì´ˆ</p>
            """
        else:
            perf = result.get('performance_metrics', {})
            html += f"""
            <p><strong>ì²˜ë¦¬ ì†ë„:</strong> {perf.get('tokens_per_second', 0):.1f} tokens/sec</p>
            <p><strong>ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:</strong> {perf.get('memory_usage_mb', 0):.0f}MB</p>
            """

        html += "</div>"

    html += """
    </body>
    </html>
    """

    return html


def generate_analysis_report(opt_results: List[Dict], bench_results: List[Dict]) -> str:
    """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
    report_path = Path("visualizations/analysis_report.html")
    Path("visualizations").mkdir(exist_ok=True)

    html = f"""
    <!DOCTYPE html>
    <html lang="ko">
    <head>
        <meta charset="UTF-8">
        <title>ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}
            .header {{ background: linear-gradient(90deg, #4CAF50, #45a049); color: white; padding: 20px; border-radius: 10px; }}
            .section {{ margin: 20px 0; padding: 15px; border-left: 4px solid #4CAF50; background-color: #f9f9f9; }}
            table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
            th, td {{ padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }}
            th {{ background-color: #4CAF50; color: white; }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>ğŸ“Š ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸</h1>
            <p>ìƒì„± ì‹œê°„: {datetime.now().strftime("%Yë…„ %mì›” %dì¼ %H:%M:%S")}</p>
        </div>

        <div class="section">
            <h2>ğŸ“ˆ ìš”ì•½</h2>
            <p>ìµœì í™” ì‹¤í—˜: {len(opt_results)}ê°œ</p>
            <p>ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸: {len(bench_results)}ê°œ</p>
        </div>
    """

    if opt_results:
        best = max(opt_results, key=lambda x: x.get('best_score', 0))
        html += f"""
        <div class="section">
            <h2>ğŸ† ìµœê³  ì„±ëŠ¥</h2>
            <p>ëª¨ë¸: {best.get('model_name', 'Unknown')}</p>
            <p>ì ìˆ˜: {best.get('best_score', 0):.3f}</p>
            <p>ë°ì´í„°ì…‹: {best.get('dataset_name', 'Unknown')}</p>
        </div>
        """

    html += """
    </body>
    </html>
    """

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(html)

    return str(report_path)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâ¹ï¸ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ì•ˆì „í•œ ì •ë¦¬ ì‘ì—…
        try:
            gc.collect()
            import torch

            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except:
            pass

        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        print("ğŸ›¡ï¸ ì•ˆì „ ëª¨ë“œë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”: --safe-mode")
        sys.exit(1)
        print("ìµœì í™”ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    # ì•ˆì „í•œ ëª¨ë¸ ë“±ë¡
    safe_model_manager = SafeModelManager(config_manager.optimization_config)
    safe_model_manager.register_model(args.model, model_config)
    optimizer.model_manager = safe_model_manager

    # ë©”ëª¨ë¦¬ ì²´í¬
    memory_usage = optimizer.resource_manager.check_memory_usage()
    print(f"   ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage}")

    # ìµœì í™” ì‹¤í–‰
    result = await optimizer.optimize_inference_params(
        model_name=args.model,
        dataset_name=args.dataset,
        evaluator_type=args.evaluator,
        optimization_strategy=args.strategy,
        max_trials=max_trials,
        num_samples=max_samples
    )

    print(f"âœ… ìµœì í™” ì™„ë£Œ!")
    print(f"   ìµœê³  ì ìˆ˜: {result.best_score:.3f}")
    print(f"   ì†Œìš” ì‹œê°„: {result.total_time:.1f}ì´ˆ")

    print(f"\nğŸ¯ ìµœì  íŒŒë¼ë¯¸í„°:")
    params = result.best_params
    print(f"   Temperature: {params.temperature:.3f}")
    print(f"   Top-p: {params.top_p:.3f}")
    print(f"   Top-k: {params.top_k}")
    print(f"   Max tokens: {params.max_new_tokens}")

    if result.recommendations:
        print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
        for i, rec in enumerate(result.recommendations, 1):
            print(f"   {i}. {rec}")

    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ë¨: optimization_results/{result.test_id}.json")

except Exception as e:
print(f"âŒ ìµœì í™” ì‹¤íŒ¨: {e}")

# ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ ì œê³µ
if "CUDA out of memory" in str(e):
    print("ğŸ’¡ í•´ê²° ë°©ë²•:")
    print("   1. --samples ìˆ˜ë¥¼ ì¤„ì—¬ë³´ì„¸ìš” (ì˜ˆ: --samples 10)")
    print("   2. --safe-mode ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”")
    print("   3. ì–‘ìí™”ë¥¼ í™œì„±í™”í•˜ì„¸ìš”")
elif "scikit-optimize" in str(e):
    print("ğŸ’¡ í•´ê²° ë°©ë²•:")
    print("   1. pip install scikit-optimize==0.9.0")
    print("   2. --strategy grid_searchë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”")
else:
    print("ğŸ’¡ ì¼ë°˜ì ì¸ í•´ê²° ë°©ë²•:")
    print("   1. --safe-mode ì˜µì…˜ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„")
    print("   2. --samples ìˆ˜ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”")
    print("   3. --debug ì˜µì…˜ìœ¼ë¡œ ìƒì„¸ ë¡œê·¸ í™•ì¸")

if args.debug:
    traceback.print_exc()
finally:
# ë©”ëª¨ë¦¬ ì •ë¦¬
gc.collect()
try:
    import torch

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
except:
    pass


async def run_benchmark_command(args):
    """benchmark ëª…ë ¹ì–´ ì‹¤í–‰ - ì•ˆì „ì„± ê°•í™”"""
    print(f"âš¡ ì•ˆì „í•œ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {args.model} on {args.dataset}")

    # ì•ˆì „í•œ ì œí•œ ì ìš©
    max_samples = min(args.samples, 100)
    max_iterations = min(args.iterations, 3)

    try:
        optimizer = SafePerformanceOptimizer()

        # ëª¨ë¸ ì„¤ì • ë° ë“±ë¡
        config_manager = RobustConfigManager()
        model_config = config_manager.get_safe_model_config(args.model)

        if not model_config:
            model_config = config_manager.create_fallback_config(args.model)

        safe_model_manager = SafeModelManager(config_manager.optimization_config)
        safe_model_manager.register_model(args.model, model_config)
        optimizer.model_manager = safe_model_manager

        # ì•ˆì „í•œ íŒŒë¼ë¯¸í„° ìƒì„±
        params = InferenceParams(
            temperature=args.temperature,
            top_p=args.top_p,
            max_new_tokens=min(args.max_tokens, 512)  # ì•ˆì „ ì œí•œ
        )

        result = await optimizer.benchmark_model(
            model_name=args.model,
            dataset_name=args.dataset,
            params=params,
            num_samples=max_samples,
            iterations=max_iterations
        )

        print(f"âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")

        perf = result.performance_metrics
        print(f"\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        print(f"   í† í°/ì´ˆ: {perf.tokens_per_second:.1f}")
        print(f"   ì§€ì—°ì‹œê°„ P95: {perf.latency_p95:.3f}ì´ˆ")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {perf.memory_usage_mb:.0f}MB")
        print(f"   ì²˜ë¦¬ëŸ‰: {perf.throughput:.1f} req/sec")

        if result.cost_analysis:
            cost = result.cost_analysis
            print(f"\nğŸ’° ë¹„ìš© ë¶„ì„:")
            print(f"   ì‹œê°„ë‹¹ ë¹„ìš©: ${cost.get('cost_per_hour_usd', 0):.4f}")
            print(f"   1Kí† í°ë‹¹ ë¹„ìš©: ${cost.get('cost_per_1k_tokens_usd', 0):.6f}")

        accuracy = len([r for r in result.evaluation_results if r.get('score', 0) > 0.8]) / len(
            result.evaluation_results)
        print(f"\nğŸ¯ ì •í™•ë„: {accuracy:.3f}")

        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ë¨: optimization_results/bench_{result.test_id}.json")

    except Exception as e:
        print(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        if args.debug:
            traceback.print_exc()
    finally:
        gc.collect()


async def run_compare_command(args):
    """compare ëª…ë ¹ì–´ ì‹¤í–‰ - ì•ˆì „ì„± ê°•í™”"""
    print(f"âš–ï¸ ì•ˆì „í•œ ëª¨ë¸ ë¹„êµ ì‹œì‘: {', '.join(args.models)} on {args.dataset}")

    max_samples = min(args.samples, 50)  # ì•ˆì „ ì œí•œ

    try:
        optimizer = SafePerformanceOptimizer()
        config_manager = RobustConfigManager()

        results = {}
        for model in args.models:
            print(f"\nğŸ”„ {model} í…ŒìŠ¤íŠ¸ ì¤‘...")

            # ì•ˆì „í•œ ëª¨ë¸ ì„¤ì •
            model_config = config_manager.get_safe_model_config(model)
            if not model_config:
                model_config = config_manager.create_fallback_config(model)

            safe_model_manager = SafeModelManager(config_manager.optimization_config)
            safe_model_manager.register_model(model, model_config)
            optimizer.model_manager = safe_model_manager

            params = InferenceParams(temperature=0.1, top_p=0.9, max_new_tokens=300)

            result = await optimizer.benchmark_model(
                model_name=model,
                dataset_name=args.dataset,
                params=params,
                num_samples=max_samples,
                iterations=1
            )

            results[model] = result

            perf = result.performance_metrics
            accuracy = len([r for r in result.evaluation_results if r.get('score', 0) > 0.8]) / len(
                result.evaluation_results)
            print(f"   âœ… ì™„ë£Œ: ì •í™•ë„ {accuracy:.3f}, {perf.tokens_per_second:.1f} tokens/sec")

        # ê²°ê³¼ ì •ë ¬ ë° ì¶œë ¥
        print(f"\nğŸ“Š ë¹„êµ ê²°ê³¼ ({args.metric} ê¸°ì¤€):")
        print(f"{'ìˆœìœ„':<4} {'ëª¨ë¸':<20} {'ì •í™•ë„':<8} {'í† í°/ì´ˆ':<10} {'ë©”ëª¨ë¦¬(MB)':<12}")
        print("-" * 60)

        # ì •ë ¬ ê¸°ì¤€ì— ë”°ë¥¸ ê²°ê³¼ ì •ë ¬
        if args.metric == 'accuracy':
            sorted_results = sorted(results.items(),
                                    key=lambda x: len(
                                        [r for r in x[1].evaluation_results if r.get('score', 0) > 0.8]) / len(
                                        x[1].evaluation_results),
                                    reverse=True)
        elif args.metric == 'speed':
            sorted_results = sorted(results.items(),
                                    key=lambda x: x[1].performance_metrics.tokens_per_second,
                                    reverse=True)
        else:
            sorted_results = list(results.items())

        for i, (model, result) in enumerate(sorted_results, 1):
            perf = result.performance_metrics
            accuracy = len([r for r in result.evaluation_results if r.get('score', 0) > 0.8]) / len(
                result.evaluation_results)

            rank_symbol = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
            print(
                f"{rank_symbol:<4} {model:<20} {accuracy:<8.3f} {perf.tokens_per_second:<10.1f} {perf.memory_usage_mb:<12.0f}")

    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¹„êµ ì‹¤íŒ¨: {e}")
        if args.debug:
            traceback.print_exc()
    finally:
        gc.collect()


def run_list_command(args):
    """list ëª…ë ¹ì–´ ì‹¤í–‰"""
    try:
        config_manager = RobustConfigManager()

        if args.type == 'models':
            models = config_manager.model_configs
            print(f"ğŸ“‹ ë“±ë¡ëœ ëª¨ë¸ ({len(models)}ê°œ):")
            for name, config in models.items():
                # ì•ˆì „ì„± ìƒíƒœ í™•ì¸
                safety_warnings = SafetyChecker.check_memory_safety(config)
                safety_status = "âš ï¸" if safety_warnings else "âœ…"

                print(f"   {safety_status} {name}")
                print(f"      ê²½ë¡œ: {config.model_path}")
                print(f"      ìœ í˜•: {config.model_type}")
                print(f"      ì¥ì¹˜: {config.device}")
                print(f"      ì–‘ìí™”: 4bit={config.load_in_4bit}, 8bit={config.load_in_8bit}")
                if safety_warnings:
                    print(f"      ê²½ê³ : {len(safety_warnings)}ê°œ ì•ˆì „ì„± ì´ìŠˆ")
                print()

        elif args.type == 'datasets':
            datasets = config_manager.test_configs
            print(f"ğŸ“‹ ë“±ë¡ëœ ë°ì´í„°ì…‹ ({len(datasets)}ê°œ):")
            for name, config in datasets.items():
                data_file = Path(config.dataset_path)
                exists = "âœ…" if data_file.exists() else "âŒ"
                print(f"   {exists} {name}")
                print(f"      ê²½ë¡œ: {config.dataset_path}")
                print(f"      ìƒ˜í”Œ: {config.num_samples}ê°œ")
                print()

        elif args.type == 'results':
            results_dir = Path("optimization_results")
            if results_dir.exists():
                result_files = list(results_dir.glob("*.json"))
                print(f"ğŸ“‹ ì €ì¥ëœ ê²°ê³¼ ({len(result_files)}ê°œ):")

                for result_file in sorted(result_files, key=lambda x: x.stat().st_mtime, reverse=True)[:10]:
                    try:
                        with open(result_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)

                        test_type = "ğŸ”§" if 'best_score' in data else "âš¡"
                        model = data.get('model_name', 'Unknown')
                        dataset = data.get('dataset_name', 'Unknown')
                        timestamp = data.get('timestamp', '')[:16].replace('T', ' ')

                        print(f"   {test_type} {result_file.name}")
                        print(f"      {model} on {dataset} ({timestamp})")

                    except Exception:
                        print(f"   âŒ {result_file.name} (ì½ê¸° ì‹¤íŒ¨)")
            else:
                print("ğŸ“‹ ì €ì¥ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")


def show_welcome_screen_enhanced():
    """ê°•í™”ëœ í™˜ì˜ í™”ë©´"""
    print_banner()
    print("ğŸš€ ì•ˆì „í•œ ì˜¤í”ˆì†ŒìŠ¤ LLM ì¶”ë¡  ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")

    print("\nğŸ“– ì•ˆì „í•œ ì‹œì‘ ê°€ì´ë“œ:")
    print("1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”: python safe_main.py init --auto-detect --safe-defaults")
    print("2. í•˜ë“œì›¨ì–´ í™•ì¸: python safe_main.py hardware")
    print("3. ëª¨ë¸ ëª©ë¡ í™•ì¸: python safe_main.py list --type models")
    print("4. ì•ˆì „í•œ ìµœì í™”: python safe_main.py optimize --model [ëª¨ë¸ëª…] --dataset [ë°ì´í„°ì…‹ëª…] --safe-mode")
    print("5. ê²°ê³¼ í™•ì¸: python safe_main.py visualize --type dashboard")

    print("\nğŸ’¡ ë„ì›€ë§:")
    print("   ì „ì²´ ëª…ë ¹ì–´: python safe_main.py --help")
    print("   ì•ˆì „ ëª¨ë“œ: ëª¨ë“  ëª…ë ¹ì–´ì— --safe-mode ì¶”ê°€")
    print("   ë¬¸ì œ í•´ê²°: --debug ì˜µì…˜ìœ¼ë¡œ ìƒì„¸ ë¡œê·¸ í™•ì¸")

    # ê°•í™”ëœ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    show_system_status_enhanced()

    print("\nğŸ¯ ê°œì„ ëœ ì£¼ìš” ê¸°ëŠ¥:")
    print("   âœ… ìŠ¤ë ˆë“œ ì•ˆì „ì„± - ë©€í‹°ìŠ¤ë ˆë“œ ì¶©ëŒ í•´ê²°")
    print("   âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬ - ìë™ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€")
    print("   âœ… ì˜¤ë¥˜ ë³µêµ¬ - ê°•í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬")
    print("   âœ… ì•ˆì „ ëª¨ë“œ - ë¦¬ì†ŒìŠ¤ ì œí•œìœ¼ë¡œ ì•ˆì •ì„± ë³´ì¥")
    print("   âœ… ì„¤ì • ê²€ì¦ - ìë™ ë¬¸ì œ ê°ì§€ ë° ìˆ˜ì •")

    print("\nâš¡ ì•ˆì „ì„± íŒ:")
    print("   ğŸ›¡ï¸ ì²˜ìŒ ì‚¬ìš©ì‹œ: --safe-mode ì˜µì…˜ í•„ìˆ˜")
    print("   ğŸ’¾ ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ: --samples 20 --trials 5 ê¶Œì¥")
    print("   ğŸ› ì˜¤ë¥˜ ë°œìƒì‹œ: --debug ì˜µì…˜ìœ¼ë¡œ ì›ì¸ íŒŒì•…")
    print("   ğŸš€ ì„±ëŠ¥ í–¥ìƒ: ì–‘ìí™” í™œì„±í™” ê¶Œì¥")


async def main():
    """ë©”ì¸ í•¨ìˆ˜ - ì•ˆì „ì„± ê°•í™”"""
    # Python ë²„ì „ í™•ì¸
    if sys.version_info < (3, 8):
        print("âŒ Python 3.8 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print(f"í˜„ì¬ ë²„ì „: {sys.version}")
        sys.exit(1)

    parser = create_argument_parser()

    # ì¸ìê°€ ì—†ìœ¼ë©´ í™˜ì˜ í™”ë©´ í‘œì‹œ
    if len(sys.argv) == 1:
        show_welcome_screen_enhanced()
        return

    try:
        args = parser.parse_args()
    except SystemExit:
        return

    if not args.command:
        print_banner()
        parser.print_help()
        return

    print_banner()

    # ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
    if hasattr(args, 'debug') and args.debug:
        import logging
        logging.basicConfig(level=logging.DEBUG)

    # ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ëª…ë ¹ì–´ë“¤
    if args.command == 'init':
        run_init_command(args)
        return

    if args.command == 'hardware':
        run_hardware_command(args)
        return

    if args.command == 'list':
        run_list_command(args)
        return

    # ì‹œê°í™” ë° ë‚´ë³´ë‚´ê¸°ëŠ” ê¸°ì¡´ ì½”ë“œ ì‚¬ìš©
    # (visualization.pyê°€ ìˆë‹¤ê³  ê°€ì •)

    # ê°•í™”ëœ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
    if not check_system_requirements_enhanced():
        print("âš ï¸ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        if not (hasattr(args, 'safe_mode') and args.safe_mode):
            response = input("ì•ˆì „ ëª¨ë“œë¡œ ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if response.lower() != 'y':  # !/usr/bin/env python3


"""
ì•ˆì „í•œ ì˜¤í”ˆì†ŒìŠ¤ LLM ì¶”ë¡  ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ë©”ì¸ CLI
ëª¨ë“  ë¬¸ì œì ì´ í•´ê²°ëœ ë²„ì „
"""
import asyncio
import argparse
import sys
import json
import traceback
import gc
import os
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime


# ì•ˆì „í•œ í™˜ê²½ ì„¤ì •
def setup_safe_environment():
    """ì•ˆì „í•œ í™˜ê²½ ì„¤ì •"""
    os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')
    os.environ.setdefault('HF_TRUST_REMOTE_CODE', 'false')
    os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'max_split_size_mb:512')
    if sys.platform.startswith('linux'):
        os.environ.setdefault('OMP_NUM_THREADS', '1')


setup_safe_environment()

# ë¡œì»¬ ëª¨ë“ˆ import
try:
    from safe_config import RobustConfigManager, ConfigValidator, HardwareDetector, SafetyChecker, InferenceParams
    from safe_test_runner import SafePerformanceOptimizer, SafeModelManager
    from visualization import ResultVisualizer
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    print("í•„ìš”í•œ íŒŒì¼ë“¤ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:")
    print("  - safe_config.py")
    print("  - safe_test_runner.py")
    print("  - safe_evaluator.py")
    print("  - visualization.py")
    print("  - dataset_loader.py")
    print("  - model_interface.py")
    sys.exit(1)


def create_argument_parser() -> argparse.ArgumentParser:
    """CLI ì¸ì íŒŒì„œ ìƒì„±"""
    parser = argparse.ArgumentParser(
        description='ì•ˆì „í•œ ì˜¤í”ˆì†ŒìŠ¤ LLM ì¶”ë¡  ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì•ˆì „í•œ ì‚¬ìš© ì˜ˆì‹œ:
  python safe_main.py init --auto-detect              # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
  python safe_main.py hardware                        # í•˜ë“œì›¨ì–´ ì •ë³´ í™•ì¸
  python safe_main.py optimize --model llama2-7b --dataset korean_math --samples 20
  python safe_main.py benchmark --model mistral-7b --dataset korean_qa --samples 50
  python safe_main.py compare --models llama2-7b mistral-7b --dataset korean_math
  python safe_main.py visualize --type dashboard      # ê²°ê³¼ ì‹œê°í™”
  python safe_main.py export --format csv             # ê²°ê³¼ ë‚´ë³´ë‚´ê¸°

ì•ˆì „ ëª¨ë“œ ì˜µì…˜:
  --safe-mode: ë©”ëª¨ë¦¬ì™€ ì„±ëŠ¥ì„ ì œí•œí•˜ì—¬ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
  --debug: ìƒì„¸í•œ ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
        """
    )

    # ì „ì—­ ì˜µì…˜
    parser.add_argument('--safe-mode', action='store_true', help='ì•ˆì „ ëª¨ë“œë¡œ ì‹¤í–‰ (ì œí•œëœ ë¦¬ì†ŒìŠ¤)')
    parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ëª¨ë“œ')

    subparsers = parser.add_subparsers(dest='command', help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´')

    # init ëª…ë ¹ì–´
    init_parser = subparsers.add_parser('init', help='ì‹œìŠ¤í…œ ì´ˆê¸°í™”')
    init_parser.add_argument('--force', action='store_true', help='ê¸°ì¡´ ì„¤ì • ë®ì–´ì“°ê¸°')
    init_parser.add_argument('--auto-detect', action='store_true', help='í•˜ë“œì›¨ì–´ ìë™ ê°ì§€')
    init_parser.add_argument('--safe-defaults', action='store_true', help='ì•ˆì „í•œ ê¸°ë³¸ê°’ ì‚¬ìš©')

    # hardware ëª…ë ¹ì–´
    hardware_parser = subparsers.add_parser('hardware', help='í•˜ë“œì›¨ì–´ ì •ë³´ í™•ì¸')
    hardware_parser.add_argument('--model-size', choices=['7b', '13b', '70b'], help='ëª¨ë¸ í¬ê¸°ë³„ ì¶”ì²œ')

    # optimize ëª…ë ¹ì–´
    optimize_parser = subparsers.add_parser('optimize', help='íŒŒë¼ë¯¸í„° ìµœì í™”')
    optimize_parser.add_argument('--model', required=True, help='ëª¨ë¸ ì´ë¦„')
    optimize_parser.add_argument('--dataset', required=True, help='ë°ì´í„°ì…‹ ì´ë¦„')
    optimize_parser.add_argument('--strategy', choices=['bayesian', 'grid_search'],
                                 default='grid_search', help='ìµœì í™” ì „ëµ')
    optimize_parser.add_argument('--trials', type=int, default=10, help='ìµœì í™” ì‹œë„ íšŸìˆ˜')
    optimize_parser.add_argument('--samples', type=int, default=20, help='í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜')
    optimize_parser.add_argument('--evaluator', default='korean_math', help='í‰ê°€ì ìœ í˜•')

    # benchmark ëª…ë ¹ì–´
    benchmark_parser = subparsers.add_parser('benchmark', help='ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬')
    benchmark_parser.add_argument('--model', required=True, help='ëª¨ë¸ ì´ë¦„')
    benchmark_parser.add_argument('--dataset', required=True, help='ë°ì´í„°ì…‹ ì´ë¦„')
    benchmark_parser.add_argument('--samples', type=int, default=50, help='í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜')
    benchmark_parser.add_argument('--iterations', type=int, default=2, help='ë°˜ë³µ íšŸìˆ˜')
    benchmark_parser.add_argument('--temperature', type=float, default=0.1, help='Temperature')
    benchmark_parser.add_argument('--top-p', type=float, default=0.9, help='Top-p')
    benchmark_parser.add_argument('--max-tokens', type=int, default=300, help='ìµœëŒ€ í† í° ìˆ˜')

    # compare ëª…ë ¹ì–´
    compare_parser = subparsers.add_parser('compare', help='ëª¨ë¸ ë¹„êµ')
    compare_parser.add_argument('--models', nargs='+', required=True, help='ë¹„êµí•  ëª¨ë¸ë“¤')
    compare_parser.add_argument('--dataset', required=True, help='ë°ì´í„°ì…‹ ì´ë¦„')
    compare_parser.add_argument('--samples', type=int, default=30, help='í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜')
    compare_parser.add_argument('--metric', choices=['accuracy', 'speed', 'efficiency'],
                                default='accuracy', help='ë¹„êµ ê¸°ì¤€')

    # list ëª…ë ¹ì–´
    list_parser = subparsers.add_parser('list', help='ì •ë³´ ì¡°íšŒ')
    list_parser.add_argument('--type', choices=['models', 'datasets', 'results'],
                             default='models', help='ì¡°íšŒí•  ì •ë³´ ìœ í˜•')

    # visualize ëª…ë ¹ì–´
    visualize_parser = subparsers.add_parser('visualize', help='ê²°ê³¼ ì‹œê°í™”')
    visualize_parser.add_argument('--type', choices=['optimization', 'benchmark', 'comparison', 'dashboard'],
                                  default='dashboard', help='ì‹œê°í™” ìœ í˜•')
    visualize_parser.add_argument('--output', help='ì¶œë ¥ íŒŒì¼ëª…')

    # export ëª…ë ¹ì–´
    export_parser = subparsers.add_parser('export', help='ê²°ê³¼ ë‚´ë³´ë‚´ê¸°')
    export_parser.add_argument('--format', choices=['csv', 'json', 'excel'],
                               default='csv', help='ë‚´ë³´ë‚´ê¸° í˜•ì‹')
    export_parser.add_argument('--output', help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')

    # analyze ëª…ë ¹ì–´
    analyze_parser = subparsers.add_parser('analyze', help='ê²°ê³¼ ë¶„ì„')
    analyze_parser.add_argument('--model', help='íŠ¹ì • ëª¨ë¸ ë¶„ì„')
    analyze_parser.add_argument('--dataset', help='íŠ¹ì • ë°ì´í„°ì…‹ ë¶„ì„')
    analyze_parser.add_argument('--report', action='store_true', help='ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±')

    return parser


def print_banner():
    """ì‹œìŠ¤í…œ ë°°ë„ˆ ì¶œë ¥"""
    banner = """
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚          ğŸš€ ì•ˆì „í•œ ì˜¤í”ˆì†ŒìŠ¤ LLM ì¶”ë¡  ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ         â”‚
â”‚              Safe Open Source LLM Optimization              â”‚
â”‚                      âœ… All Issues Fixed                     â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
"""
    print(banner)


def check_system_requirements_enhanced() -> bool:
    """ê°•í™”ëœ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
    requirements_met = True
    issues = []

    # Python ë²„ì „ í™•ì¸
    if sys.version_info < (3, 8):
        issues.append(f"Python 3.8+ í•„ìš” (í˜„ì¬: {sys.version})")
        requirements_met = False

    # í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
    required_packages = {
        'torch': 'PyTorch',
        'transformers': 'HuggingFace Transformers',
        'numpy': 'NumPy',
        'pandas': 'Pandas'
    }

    missing_packages = []
    for package, description in required_packages.items():
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(f"{package} ({description})")

    if missing_packages:
        issues.append(f"ëˆ„ë½ëœ í•„ìˆ˜ íŒ¨í‚¤ì§€: {', '.join(missing_packages)}")
        requirements_met = False

    # ì„ íƒì  íŒ¨í‚¤ì§€ í™•ì¸
    optional_packages = {
        'skopt': 'scikit-optimize (ë² ì´ì§€ì•ˆ ìµœì í™”ìš©)',
        'plotly': 'Plotly (ì‹œê°í™”ìš©)',
        'sentence_transformers': 'SentenceTransformers (ì˜ë¯¸ì  ìœ ì‚¬ë„ìš©)'
    }

    missing_optional = []
    for package, description in optional_packages.items():
        try:
            __import__(package)
        except ImportError:
            missing_optional.append(f"{package} ({description})")

    if missing_optional:
        print(f"âš ï¸ ì„ íƒì  íŒ¨í‚¤ì§€ ëˆ„ë½: {', '.join(missing_optional)}")
        print("  ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # GPU ê´€ë ¨ í™•ì¸
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            total_memory = sum(
                torch.cuda.get_device_properties(i).total_memory // (1024 ** 3)
                for i in range(gpu_count)
            )
            print(f"âœ… GPU ê°ì§€: {gpu_count}ê°œ, ì´ ë©”ëª¨ë¦¬: {total_memory}GB")

            if total_memory < 8:
                print("âš ï¸ GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–‘ìí™” ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        else:
            print("âš ï¸ CUDA ì‚¬ìš©