#!/usr/bin/env python3
"""
ì˜¤í”ˆì†ŒìŠ¤ LLM ì¶”ë¡  ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ë©”ì¸ CLI
"""
import asyncio
import argparse
import sys
import json
import traceback
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# ë¡œì»¬ ëª¨ë“ˆ import ì‹œë„
try:
    from config import ConfigManager, InferenceParams, HardwareDetector
    from test_runner import PerformanceOptimizer
    from visualization import ResultVisualizer
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    print("í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    print("ë˜ëŠ” python -m pip install -r requirements.txt ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    sys.exit(1)

def create_argument_parser() -> argparse.ArgumentParser:
    """CLI ì¸ì íŒŒì„œ ìƒì„±"""
    parser = argparse.ArgumentParser(
        description='ì˜¤í”ˆì†ŒìŠ¤ LLM ì¶”ë¡  ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python main.py init --auto-detect              # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
  python main.py hardware                        # í•˜ë“œì›¨ì–´ ì •ë³´ í™•ì¸
  python main.py optimize --model llama2-7b --dataset korean_math
  python main.py benchmark --model mistral-7b --dataset korean_qa
  python main.py compare --models llama2-7b mistral-7b --dataset korean_math
  python main.py visualize --type dashboard      # ê²°ê³¼ ì‹œê°í™”
  python main.py export --format csv             # ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
        """
    )

    subparsers = parser.add_subparsers(dest='command', help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´')

    # init ëª…ë ¹ì–´
    init_parser = subparsers.add_parser('init', help='ì‹œìŠ¤í…œ ì´ˆê¸°í™”')
    init_parser.add_argument('--force', action='store_true', help='ê¸°ì¡´ ì„¤ì • ë®ì–´ì“°ê¸°')
    init_parser.add_argument('--auto-detect', action='store_true', help='í•˜ë“œì›¨ì–´ ìë™ ê°ì§€')

    # hardware ëª…ë ¹ì–´
    hardware_parser = subparsers.add_parser('hardware', help='í•˜ë“œì›¨ì–´ ì •ë³´ í™•ì¸')
    hardware_parser.add_argument('--model-size', choices=['7b', '13b', '70b'], help='ëª¨ë¸ í¬ê¸°ë³„ ì¶”ì²œ')

    # optimize ëª…ë ¹ì–´
    optimize_parser = subparsers.add_parser('optimize', help='íŒŒë¼ë¯¸í„° ìµœì í™”')
    optimize_parser.add_argument('--model', required=True, help='ëª¨ë¸ ì´ë¦„')
    optimize_parser.add_argument('--dataset', required=True, help='ë°ì´í„°ì…‹ ì´ë¦„')
    optimize_parser.add_argument('--strategy', choices=['bayesian', 'grid_search', 'evolutionary'],
                                default='bayesian', help='ìµœì í™” ì „ëµ')
    optimize_parser.add_argument('--trials', type=int, default=20, help='ìµœì í™” ì‹œë„ íšŸìˆ˜')
    optimize_parser.add_argument('--samples', type=int, default=50, help='í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜')
    optimize_parser.add_argument('--evaluator', default='korean_math', help='í‰ê°€ì ìœ í˜•')

    # benchmark ëª…ë ¹ì–´
    benchmark_parser = subparsers.add_parser('benchmark', help='ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬')
    benchmark_parser.add_argument('--model', required=True, help='ëª¨ë¸ ì´ë¦„')
    benchmark_parser.add_argument('--dataset', required=True, help='ë°ì´í„°ì…‹ ì´ë¦„')
    benchmark_parser.add_argument('--samples', type=int, default=100, help='í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜')
    benchmark_parser.add_argument('--iterations', type=int, default=3, help='ë°˜ë³µ íšŸìˆ˜')
    benchmark_parser.add_argument('--temperature', type=float, default=0.1, help='Temperature')
    benchmark_parser.add_argument('--top-p', type=float, default=0.9, help='Top-p')
    benchmark_parser.add_argument('--max-tokens', type=int, default=512, help='ìµœëŒ€ í† í° ìˆ˜')

    # compare ëª…ë ¹ì–´
    compare_parser = subparsers.add_parser('compare', help='ëª¨ë¸ ë¹„êµ')
    compare_parser.add_argument('--models', nargs='+', required=True, help='ë¹„êµí•  ëª¨ë¸ë“¤')
    compare_parser.add_argument('--dataset', required=True, help='ë°ì´í„°ì…‹ ì´ë¦„')
    compare_parser.add_argument('--samples', type=int, default=100, help='í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜')
    compare_parser.add_argument('--metric', choices=['accuracy', 'speed', 'efficiency'],
                               default='accuracy', help='ë¹„êµ ê¸°ì¤€')

    # list ëª…ë ¹ì–´
    list_parser = subparsers.add_parser('list', help='ì •ë³´ ì¡°íšŒ')
    list_parser.add_argument('--type', choices=['models', 'datasets', 'results'],
                           default='models', help='ì¡°íšŒí•  ì •ë³´ ìœ í˜•')

    # visualize ëª…ë ¹ì–´
    visualize_parser = subparsers.add_parser('visualize', help='ê²°ê³¼ ì‹œê°í™”')
    visualize_parser.add_argument('--type', choices=['optimization', 'benchmark', 'comparison', 'dashboard'],
                                 default='dashboard', help='ì‹œê°í™” ìœ í˜•')
    visualize_parser.add_argument('--output', help='ì¶œë ¥ íŒŒì¼ëª…')

    # export ëª…ë ¹ì–´
    export_parser = subparsers.add_parser('export', help='ê²°ê³¼ ë‚´ë³´ë‚´ê¸°')
    export_parser.add_argument('--format', choices=['csv', 'json', 'excel'], 
                              default='csv', help='ë‚´ë³´ë‚´ê¸° í˜•ì‹')
    export_parser.add_argument('--output', help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')

    # analyze ëª…ë ¹ì–´
    analyze_parser = subparsers.add_parser('analyze', help='ê²°ê³¼ ë¶„ì„')
    analyze_parser.add_argument('--model', help='íŠ¹ì • ëª¨ë¸ ë¶„ì„')
    analyze_parser.add_argument('--dataset', help='íŠ¹ì • ë°ì´í„°ì…‹ ë¶„ì„')
    analyze_parser.add_argument('--report', action='store_true', help='ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±')

    return parser

def print_banner():
    """ì‹œìŠ¤í…œ ë°°ë„ˆ ì¶œë ¥"""
    banner = """
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚              ğŸš€ ì˜¤í”ˆì†ŒìŠ¤ LLM ì¶”ë¡  ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ           â”‚
â”‚                 Open Source LLM Optimization                â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
"""
    print(banner)

def check_system_requirements() -> bool:
    """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
    requirements_met = True

    # Python ë²„ì „ í™•ì¸
    if sys.version_info < (3, 8):
        print(f"âŒ Python 3.8 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: {sys.version}")
        requirements_met = False

    # í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
    required_packages = ['torch', 'transformers', 'numpy', 'pandas']
    missing_packages = []

    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)

    if missing_packages:
        print(f"âŒ ëˆ„ë½ëœ íŒ¨í‚¤ì§€: {', '.join(missing_packages)}")
        print(f"ì„¤ì¹˜ ëª…ë ¹: pip install {' '.join(missing_packages)}")
        requirements_met = False

    return requirements_met

def show_system_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ"""
    print("ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸:")

    # ì„¤ì • íŒŒì¼ í™•ì¸
    config_file = Path("llm_config.json")
    if config_file.exists():
        print("   âœ… ì„¤ì • íŒŒì¼ ì¡´ì¬")
    else:
        print("   âŒ ì„¤ì • íŒŒì¼ ì—†ìŒ (python main.py init ì‹¤í–‰ í•„ìš”)")

    # ë””ë ‰í† ë¦¬ í™•ì¸
    required_dirs = ["data", "optimization_results", "visualizations"]
    for dir_name in required_dirs:
        if Path(dir_name).exists():
            print(f"   âœ… {dir_name} ë””ë ‰í† ë¦¬ ì¡´ì¬")
        else:
            print(f"   âŒ {dir_name} ë””ë ‰í† ë¦¬ ì—†ìŒ")

    # í•˜ë“œì›¨ì–´ ì •ë³´
    try:
        hardware_info = HardwareDetector.detect_hardware()
        print(f"   ğŸ’» GPU: {hardware_info['cuda_device_count']}ê°œ")
        print(f"   ğŸ’¾ ë©”ëª¨ë¦¬: {hardware_info['available_memory']}/{hardware_info['total_memory']}GB")
    except Exception:
        print("   âš ï¸  í•˜ë“œì›¨ì–´ ì •ë³´ í™•ì¸ ë¶ˆê°€")

    # ê²°ê³¼ íŒŒì¼ í™•ì¸
    results_dir = Path("optimization_results")
    if results_dir.exists():
        result_count = len(list(results_dir.glob("*.json")))
        print(f"   ğŸ“Š ì €ì¥ëœ ê²°ê³¼: {result_count}ê°œ")
    else:
        print("   ğŸ“Š ì €ì¥ëœ ê²°ê³¼: 0ê°œ")

def run_init_command(args):
    """init ëª…ë ¹ì–´ ì‹¤í–‰"""
    print("ğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")

    config_file = "llm_config.json"

    if Path(config_file).exists() and not args.force:
        print(f"âš ï¸  ì„¤ì • íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {config_file}")
        print("   ë®ì–´ì“°ë ¤ë©´ --force ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        return

    try:
        # ì„¤ì • ë§¤ë‹ˆì € ìƒì„±
        config_manager = ConfigManager(config_file)
        print(f"âœ… ì„¤ì • íŒŒì¼ ìƒì„±: {config_file}")

        # ë””ë ‰í† ë¦¬ ìƒì„±
        directories = ["data", "optimization_results", "visualizations"]
        for directory in directories:
            Path(directory).mkdir(exist_ok=True)
            print(f"   âœ… {directory} ë””ë ‰í† ë¦¬ ìƒì„±")

        # í•˜ë“œì›¨ì–´ ìë™ ê°ì§€
        if args.auto_detect:
            print("ğŸ” í•˜ë“œì›¨ì–´ ìë™ ê°ì§€ ì¤‘...")
            try:
                hardware_info = HardwareDetector.detect_hardware()
                print(f"   GPU: {hardware_info['cuda_device_count']}ê°œ")
                print(f"   ë©”ëª¨ë¦¬: {hardware_info['total_memory']}GB")
                if hardware_info['cuda_available']:
                    total_gpu_memory = sum(
                        hardware_info.get(f'gpu_{i}_memory', 0) 
                        for i in range(hardware_info['cuda_device_count'])
                    )
                    print(f"   GPU ë©”ëª¨ë¦¬: {total_gpu_memory}GB")
            except Exception as e:
                print(f"âš ï¸  í•˜ë“œì›¨ì–´ ê°ì§€ ì‹¤íŒ¨: {e}")

        print("\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. í•˜ë“œì›¨ì–´ ì •ë³´ í™•ì¸: python main.py hardware")
        print("2. ëª¨ë¸ ëª©ë¡ í™•ì¸: python main.py list --type models")
        print("3. ìµœì í™” ì‹¤í–‰: python main.py optimize --model [ëª¨ë¸ëª…] --dataset [ë°ì´í„°ì…‹ëª…]")

    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        sys.exit(1)

def run_hardware_command(args):
    """hardware ëª…ë ¹ì–´ ì‹¤í–‰"""
    print("ğŸ’» í•˜ë“œì›¨ì–´ ì •ë³´ ë¶„ì„")

    try:
        hardware_info = HardwareDetector.detect_hardware()

        print(f"\nğŸ” ê°ì§€ëœ í•˜ë“œì›¨ì–´:")
        print(f"   CUDA ì‚¬ìš© ê°€ëŠ¥: {'âœ…' if hardware_info['cuda_available'] else 'âŒ'}")
        print(f"   GPU ê°œìˆ˜: {hardware_info['cuda_device_count']}")
        print(f"   ì´ ë©”ëª¨ë¦¬: {hardware_info['total_memory']}GB")
        print(f"   ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {hardware_info['available_memory']}GB")
        print(f"   CPU ì½”ì–´: {hardware_info['cpu_cores']}")

        if hardware_info['cuda_available']:
            total_gpu_memory = 0
            for i in range(hardware_info['cuda_device_count']):
                gpu_memory = hardware_info.get(f'gpu_{i}_memory', 0)
                gpu_name = hardware_info.get(f'gpu_{i}_name', 'Unknown')
                print(f"   GPU {i}: {gpu_name} ({gpu_memory}GB)")
                total_gpu_memory += gpu_memory

            print(f"\nğŸ¯ ëª¨ë¸ ì¶”ì²œ:")
            if total_gpu_memory >= 80:
                print("   âœ… 70B ëª¨ë¸ê¹Œì§€ ì‹¤í–‰ ê°€ëŠ¥")
            elif total_gpu_memory >= 32:
                print("   âœ… 13B ëª¨ë¸ê¹Œì§€ ì‹¤í–‰ ê°€ëŠ¥")
            elif total_gpu_memory >= 16:
                print("   âœ… 7B ëª¨ë¸ ì‹¤í–‰ ê°€ëŠ¥ (float16)")
            elif total_gpu_memory >= 8:
                print("   âš ï¸  7B ëª¨ë¸ ì‹¤í–‰ ê°€ëŠ¥ (4-bit ì–‘ìí™” í•„ìš”)")
            else:
                print("   âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, CPU ì¶”ë¡  ê¶Œì¥")

        # ëª¨ë¸ í¬ê¸°ë³„ ì¶”ì²œ ì„¤ì •
        if hasattr(args, 'model_size') and args.model_size:
            print(f"\nğŸ¯ {args.model_size.upper()} ëª¨ë¸ ì¶”ì²œ ì„¤ì •:")
            recommended = HardwareDetector.recommend_config(args.model_size)
            print(f"   ì¥ì¹˜: {recommended.device}")
            print(f"   ë°ì´í„° íƒ€ì…: {recommended.dtype}")
            print(f"   4-bit ì–‘ìí™”: {'âœ…' if recommended.load_in_4bit else 'âŒ'}")
            print(f"   8-bit ì–‘ìí™”: {'âœ…' if recommended.load_in_8bit else 'âŒ'}")

    except Exception as e:
        print(f"âŒ í•˜ë“œì›¨ì–´ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")

async def run_optimize_command(args):
    """optimize ëª…ë ¹ì–´ ì‹¤í–‰"""
    print(f"ğŸ”§ íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘: {args.model} on {args.dataset}")
    print(f"   ì „ëµ: {args.strategy}, ì‹œë„: {args.trials}íšŒ, ìƒ˜í”Œ: {args.samples}ê°œ")

    try:
        optimizer = PerformanceOptimizer()
        optimizer.setup_models()

        result = await optimizer.optimize_inference_params(
            model_name=args.model,
            dataset_name=args.dataset,
            evaluator_type=args.evaluator,
            optimization_strategy=args.strategy,
            max_trials=args.trials,
            num_samples=args.samples
        )

        print(f"âœ… ìµœì í™” ì™„ë£Œ!")
        print(f"   ìµœê³  ì ìˆ˜: {result.best_score:.3f}")
        print(f"   ì†Œìš” ì‹œê°„: {result.total_time:.1f}ì´ˆ")

        print(f"\nğŸ¯ ìµœì  íŒŒë¼ë¯¸í„°:")
        params = result.best_params
        print(f"   Temperature: {params.temperature:.3f}")
        print(f"   Top-p: {params.top_p:.3f}")
        print(f"   Top-k: {params.top_k}")
        print(f"   Max tokens: {params.max_new_tokens}")

        if result.recommendations:
            print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
            for i, rec in enumerate(result.recommendations, 1):
                print(f"   {i}. {rec}")

        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ë¨: optimization_results/{result.test_id}.json")

    except Exception as e:
        print(f"âŒ ìµœì í™” ì‹¤íŒ¨: {e}")
        traceback.print_exc()

async def run_benchmark_command(args):
    """benchmark ëª…ë ¹ì–´ ì‹¤í–‰"""
    print(f"âš¡ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {args.model} on {args.dataset}")

    try:
        optimizer = PerformanceOptimizer()
        optimizer.setup_models()

        params = InferenceParams(
            temperature=args.temperature,
            top_p=args.top_p,
            max_new_tokens=args.max_tokens
        )

        result = await optimizer.benchmark_model(
            model_name=args.model,
            dataset_name=args.dataset,
            params=params,
            num_samples=args.samples,
            iterations=args.iterations
        )

        print(f"âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")

        perf = result.performance_metrics
        print(f"\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        print(f"   í† í°/ì´ˆ: {perf.tokens_per_second:.1f}")
        print(f"   ì§€ì—°ì‹œê°„ P95: {perf.latency_p95:.3f}ì´ˆ")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {perf.memory_usage_mb:.0f}MB")
        print(f"   ì²˜ë¦¬ëŸ‰: {perf.throughput:.1f} req/sec")

        if result.cost_analysis:
            cost = result.cost_analysis
            print(f"\nğŸ’° ë¹„ìš© ë¶„ì„:")
            print(f"   ì‹œê°„ë‹¹ ë¹„ìš©: ${cost.get('cost_per_hour_usd', 0):.4f}")
            print(f"   1Kí† í°ë‹¹ ë¹„ìš©: ${cost.get('cost_per_1k_tokens_usd', 0):.6f}")

        accuracy = sum(1 for r in result.evaluation_results if r.correct) / len(result.evaluation_results)
        print(f"\nğŸ¯ ì •í™•ë„: {accuracy:.3f}")

        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ë¨: optimization_results/bench_{result.test_id}.json")

    except Exception as e:
        print(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        traceback.print_exc()

async def run_compare_command(args):
    """compare ëª…ë ¹ì–´ ì‹¤í–‰"""
    print(f"âš–ï¸ ëª¨ë¸ ë¹„êµ ì‹œì‘: {', '.join(args.models)} on {args.dataset}")

    try:
        optimizer = PerformanceOptimizer()
        optimizer.setup_models()

        results = {}
        for model in args.models:
            print(f"\nğŸ”„ {model} í…ŒìŠ¤íŠ¸ ì¤‘...")

            params = InferenceParams(temperature=0.1, top_p=0.9, max_new_tokens=512)

            result = await optimizer.benchmark_model(
                model_name=model,
                dataset_name=args.dataset,
                params=params,
                num_samples=args.samples,
                iterations=1
            )

            results[model] = result

            perf = result.performance_metrics
            accuracy = sum(1 for r in result.evaluation_results if r.correct) / len(result.evaluation_results)
            print(f"   âœ… ì™„ë£Œ: ì •í™•ë„ {accuracy:.3f}, {perf.tokens_per_second:.1f} tokens/sec")

        # ê²°ê³¼ ì •ë ¬ ë° ì¶œë ¥
        print(f"\nğŸ“Š ë¹„êµ ê²°ê³¼ ({args.metric} ê¸°ì¤€):")
        print(f"{'ìˆœìœ„':<4} {'ëª¨ë¸':<20} {'ì •í™•ë„':<8} {'í† í°/ì´ˆ':<10} {'ë©”ëª¨ë¦¬(MB)':<12}")
        print("-" * 60)

        if args.metric == 'accuracy':
            sorted_results = sorted(results.items(), 
                                  key=lambda x: sum(1 for r in x[1].evaluation_results if r.correct) / len(x[1].evaluation_results),
                                  reverse=True)
        elif args.metric == 'speed':
            sorted_results = sorted(results.items(), 
                                  key=lambda x: x[1].performance_metrics.tokens_per_second,
                                  reverse=True)
        else:
            sorted_results = list(results.items())

        for i, (model, result) in enumerate(sorted_results, 1):
            perf = result.performance_metrics
            accuracy = sum(1 for r in result.evaluation_results if r.correct) / len(result.evaluation_results)

            rank_symbol = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
            print(f"{rank_symbol:<4} {model:<20} {accuracy:<8.3f} {perf.tokens_per_second:<10.1f} {perf.memory_usage_mb:<12.0f}")

    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¹„êµ ì‹¤íŒ¨: {e}")
        traceback.print_exc()

def run_list_command(args):
    """list ëª…ë ¹ì–´ ì‹¤í–‰"""
    try:
        config_manager = ConfigManager()

        if args.type == 'models':
            models = config_manager.model_configs
            print(f"ğŸ“‹ ë“±ë¡ëœ ëª¨ë¸ ({len(models)}ê°œ):")
            for name, config in models.items():
                print(f"   ğŸ“¦ {name}")
                print(f"      ê²½ë¡œ: {config.model_path}")
                print(f"      ìœ í˜•: {config.model_type}")
                print(f"      ì¥ì¹˜: {config.device}")
                print(f"      íƒ€ì…: {config.dtype}")
                print()

        elif args.type == 'datasets':
            datasets = config_manager.test_configs
            print(f"ğŸ“‹ ë“±ë¡ëœ ë°ì´í„°ì…‹ ({len(datasets)}ê°œ):")
            for name, config in datasets.items():
                print(f"   ğŸ“„ {name}")
                print(f"      ê²½ë¡œ: {config.dataset_path}")
                print(f"      ìƒ˜í”Œ: {config.num_samples}ê°œ")
                print()

        elif args.type == 'results':
            results_dir = Path("optimization_results")
            if results_dir.exists():
                result_files = list(results_dir.glob("*.json"))
                print(f"ğŸ“‹ ì €ì¥ëœ ê²°ê³¼ ({len(result_files)}ê°œ):")

                for result_file in sorted(result_files, key=lambda x: x.stat().st_mtime, reverse=True)[:10]:
                    try:
                        with open(result_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)

                        test_type = "ğŸ”§" if 'best_score' in data else "âš¡"
                        model = data.get('model_name', 'Unknown')
                        dataset = data.get('dataset_name', 'Unknown')
                        timestamp = data.get('timestamp', '')[:16].replace('T', ' ')

                        print(f"   {test_type} {result_file.name}")
                        print(f"      {model} on {dataset} ({timestamp})")

                    except Exception:
                        print(f"   âŒ {result_file.name} (ì½ê¸° ì‹¤íŒ¨)")
            else:
                print("ğŸ“‹ ì €ì¥ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

def run_visualize_command(args):
    """visualize ëª…ë ¹ì–´ ì‹¤í–‰"""
    print(f"ğŸ“Š ì‹œê°í™” ìƒì„±: {args.type}")

    try:
        visualizer = ResultVisualizer("optimization_results")
        output_name = args.output or f"{args.type}_output"

        if args.type == 'optimization':
            results = visualizer.load_optimization_results()
            if results:
                fig = visualizer.create_optimization_analysis(results, output_name)
                print(f"âœ… ìµœì í™” ë¶„ì„ ì°¨íŠ¸ ìƒì„±: visualizations/{output_name}.html")
            else:
                print("âŒ ìµœì í™” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        elif args.type == 'benchmark':
            results = visualizer.load_benchmark_results()
            if results:
                fig = visualizer.create_benchmark_analysis(results, output_name)
                print(f"âœ… ë²¤ì¹˜ë§ˆí¬ ë¶„ì„ ì°¨íŠ¸ ìƒì„±: visualizations/{output_name}.html")
            else:
                print("âŒ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        elif args.type == 'comparison':
            results = visualizer.load_all_results()
            if results:
                fig = visualizer.create_model_comparison_chart(results, output_name)
                print(f"âœ… ëª¨ë¸ ë¹„êµ ì°¨íŠ¸ ìƒì„±: visualizations/{output_name}.html")
            else:
                print("âŒ ë¹„êµí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        elif args.type == 'dashboard':
            fig = visualizer.create_comprehensive_dashboard(output_name)
            print(f"âœ… ì¢…í•© ëŒ€ì‹œë³´ë“œ ìƒì„±: visualizations/{output_name}.html")

    except Exception as e:
        print(f"âŒ ì‹œê°í™” ì‹¤íŒ¨: {e}")
        traceback.print_exc()

def run_export_command(args):
    """export ëª…ë ¹ì–´ ì‹¤í–‰"""
    print(f"ğŸ“¤ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°: {args.format} í˜•ì‹")

    try:
        results_dir = Path("optimization_results")
        if not results_dir.exists():
            print("âŒ ë‚´ë³´ë‚¼ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        all_results = []
        for result_file in results_dir.glob("*.json"):
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                flat_data = {
                    'file_name': result_file.name,
                    'test_id': data.get('test_id', ''),
                    'model_name': data.get('model_name', ''),
                    'dataset_name': data.get('dataset_name', ''),
                    'timestamp': data.get('timestamp', ''),
                    'test_type': 'optimization' if 'best_score' in data else 'benchmark'
                }

                if 'best_score' in data:
                    flat_data.update({
                        'best_score': data.get('best_score', 0),
                        'total_time': data.get('total_time', 0),
                        'temperature': data.get('best_params', {}).get('temperature', 0),
                        'top_p': data.get('best_params', {}).get('top_p', 0),
                        'top_k': data.get('best_params', {}).get('top_k', 0)
                    })

                elif 'performance_metrics' in data:
                    perf = data.get('performance_metrics', {})
                    flat_data.update({
                        'tokens_per_second': perf.get('tokens_per_second', 0),
                        'latency_p95': perf.get('latency_p95', 0),
                        'memory_usage_mb': perf.get('memory_usage_mb', 0)
                    })

                all_results.append(flat_data)

            except Exception as e:
                print(f"âš ï¸  íŒŒì¼ {result_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        if not all_results:
            print("âŒ ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ì¶œë ¥ íŒŒì¼ëª… ê²°ì •
        if args.output:
            output_path = Path(args.output)
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = Path(f"llm_results_{timestamp}.{args.format}")

        # í˜•ì‹ë³„ ë‚´ë³´ë‚´ê¸°
        if args.format == 'json':
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, indent=2, ensure_ascii=False)

        elif args.format == 'csv':
            try:
                import pandas as pd
                df = pd.DataFrame(all_results)
                df.to_csv(output_path, index=False, encoding='utf-8-sig')
            except ImportError:
                print("âŒ pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install pandas")
                return

        elif args.format == 'excel':
            try:
                import pandas as pd
                df = pd.DataFrame(all_results)
                df.to_excel(output_path, index=False)
            except ImportError:
                print("âŒ pandasì™€ openpyxlì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install pandas openpyxl")
                return

        print(f"âœ… ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")
        print(f"   ì´ {len(all_results)}ê°œ ê²°ê³¼ ë‚´ë³´ëƒ„")

    except Exception as e:
        print(f"âŒ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        traceback.print_exc()

def run_analyze_command(args):
    """analyze ëª…ë ¹ì–´ ì‹¤í–‰"""
    print("ğŸ” ê²°ê³¼ ë¶„ì„ ì‹œì‘...")

    try:
        results_dir = Path("optimization_results")
        if not results_dir.exists():
            print("âŒ ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        optimization_results = []
        benchmark_results = []

        for result_file in results_dir.glob("*.json"):
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # í•„í„° ì ìš©
                if args.model and args.model.lower() not in data.get('model_name', '').lower():
                    continue
                if args.dataset and args.dataset.lower() not in data.get('dataset_name', '').lower():
                    continue

                if 'best_score' in data:
                    optimization_results.append(data)
                elif 'performance_metrics' in data:
                    benchmark_results.append(data)

            except Exception:
                continue

        print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ: ìµœì í™” {len(optimization_results)}ê°œ, ë²¤ì¹˜ë§ˆí¬ {len(benchmark_results)}ê°œ")

        # ìµœì í™” ê²°ê³¼ ë¶„ì„
        if optimization_results:
            print(f"\nğŸ”§ ìµœì í™” ê²°ê³¼ ë¶„ì„:")
            best_result = max(optimization_results, key=lambda x: x.get('best_score', 0))
            avg_score = sum(r.get('best_score', 0) for r in optimization_results) / len(optimization_results)

            print(f"   ìµœê³  ì ìˆ˜: {best_result.get('best_score', 0):.3f} ({best_result.get('model_name', 'Unknown')})")
            print(f"   í‰ê·  ì ìˆ˜: {avg_score:.3f}")

            best_params = best_result.get('best_params', {})
            print(f"   ìµœì  íŒŒë¼ë¯¸í„°:")
            print(f"     Temperature: {best_params.get('temperature', 0):.3f}")
            print(f"     Top-p: {best_params.get('top_p', 0):.3f}")
            print(f"     Top-k: {best_params.get('top_k', 0)}")

        # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„
        if benchmark_results:
            print(f"\nâš¡ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„:")
            fastest_model = max(benchmark_results, 
                              key=lambda x: x.get('performance_metrics', {}).get('tokens_per_second', 0))

            fastest_speed = fastest_model.get('performance_metrics', {}).get('tokens_per_second', 0)
            print(f"   ìµœê³  ì†ë„: {fastest_speed:.1f} tokens/sec ({fastest_model.get('model_name', 'Unknown')})")

        print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
        print("   1. ì •í™•ë„ì™€ ì†ë„ì˜ ê· í˜•ì„ ê³ ë ¤í•œ íŒŒë¼ë¯¸í„° ì‚¬ìš©")
        print("   2. ì •ê¸°ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
        print("   3. í•˜ë“œì›¨ì–´ ì‚¬ìš©ëŸ‰ ìµœì í™”")

        # ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
        if args.report:
            try:
                visualizer = ResultVisualizer("optimization_results")
                report_path = visualizer.generate_performance_report("analysis_report.html")
                print(f"\nğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±: {report_path}")
            except Exception as e:
                print(f"âš ï¸  ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")

    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        traceback.print_exc()

def show_welcome_screen():
    """í™˜ì˜ í™”ë©´ í‘œì‹œ"""
    print_banner()
    print("ğŸš€ ì˜¤í”ˆì†ŒìŠ¤ LLM ì¶”ë¡  ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")

    print("\nğŸ“– ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ:")
    print("1. ì‹œìŠ¤í…œ ì´ˆê¸°í™”: python main.py init --auto-detect")
    print("2. í•˜ë“œì›¨ì–´ í™•ì¸: python main.py hardware")
    print("3. ëª¨ë¸ ëª©ë¡ í™•ì¸: python main.py list --type models")
    print("4. ìµœì í™” ì‹¤í–‰: python main.py optimize --model [ëª¨ë¸ëª…] --dataset [ë°ì´í„°ì…‹ëª…]")
    print("5. ê²°ê³¼ í™•ì¸: python main.py visualize --type dashboard")

    print("\nğŸ’¡ ë„ì›€ë§:")
    print("   ì „ì²´ ëª…ë ¹ì–´: python main.py --help")
    print("   íŠ¹ì • ëª…ë ¹ì–´: python main.py [ëª…ë ¹ì–´] --help")

    # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    show_system_status()

    print("\nğŸ¯ ì£¼ìš” ê¸°ëŠ¥:")
    print("   ğŸ”§ íŒŒë¼ë¯¸í„° ìµœì í™” - ë² ì´ì§€ì•ˆ/ê·¸ë¦¬ë“œì„œì¹˜/ì§„í™” ì•Œê³ ë¦¬ì¦˜")
    print("   âš¡ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ - ì†ë„/ë©”ëª¨ë¦¬/ë¹„ìš© ë¶„ì„")
    print("   âš–ï¸ ëª¨ë¸ ë¹„êµ - ë‹¤ì¤‘ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
    print("   ğŸ“Š ê²°ê³¼ ì‹œê°í™” - ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸ ë° ëŒ€ì‹œë³´ë“œ")
    print("   ğŸ“¤ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° - CSV/JSON/Excel í˜•ì‹ ì§€ì›")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # Python ë²„ì „ í™•ì¸
    if sys.version_info < (3, 8):
        print("âŒ Python 3.8 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print(f"í˜„ì¬ ë²„ì „: {sys.version}")
        sys.exit(1)

    parser = create_argument_parser()

    # ì¸ìê°€ ì—†ìœ¼ë©´ í™˜ì˜ í™”ë©´ í‘œì‹œ
    if len(sys.argv) == 1:
        show_welcome_screen()
        return

    try:
        args = parser.parse_args()
    except SystemExit:
        return

    if not args.command:
        print_banner()
        parser.print_help()
        return

    print_banner()

    # ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ëª…ë ¹ì–´ë“¤
    if args.command == 'init':
        run_init_command(args)
        return

    if args.command == 'hardware':
        run_hardware_command(args)
        return

    if args.command == 'list':
        run_list_command(args)
        return

    if args.command == 'visualize':
        run_visualize_command(args)
        return

    if args.command == 'export':
        run_export_command(args)
        return

    if args.command == 'analyze':
        run_analyze_command(args)
        return

    # ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
    if not check_system_requirements():
        print("âš ï¸  ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N)")
        if input().lower() != 'y':
            sys.exit(1)

    # PerformanceOptimizerê°€ í•„ìš”í•œ ëª…ë ¹ì–´ë“¤
    try:
        print("ğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

        # PerformanceOptimizer ì‹¤í–‰ ì „ ì„¤ì • íŒŒì¼ í™•ì¸
        config_file = Path("llm_config.json")
        if not config_file.exists():
            print("âŒ ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("   python main.py init ëª…ë ¹ì–´ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return

        # PerformanceOptimizer ì´ˆê¸°í™”ëŠ” ê° ëª…ë ¹ì–´ì—ì„œ ìˆ˜í–‰
        if args.command == 'optimize':
            await run_optimize_command(args)
        elif args.command == 'benchmark':
            await run_benchmark_command(args)
        elif args.command == 'compare':
            await run_compare_command(args)
        else:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {args.command}")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        if "--debug" in sys.argv:
            traceback.print_exc()
        else:
            print("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´: python main.py [ëª…ë ¹ì–´] --debug")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâ¹ï¸ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        if "--debug" in sys.argv:
            traceback.print_exc()
        sys.exit(1)